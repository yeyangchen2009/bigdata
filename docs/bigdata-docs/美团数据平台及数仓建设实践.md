**涵盖数据平台、数仓开发、数据治理、数据分析等**

**本文档整理自美团技术团队博客：[https://tech.meituan.com](https://tech.meituan.com/)**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image1.jpeg)

# 离线数仓

### 一、美团外卖离线数仓建设实践

导读：美团外卖数据仓库主要是收集各种用户终端业务、行为数据，通过统一口径加工处理，通过多种数据服务支撑主题报表、数据分析等多种方式的应用。数据组作为数据基础部门，支持用户端、商家端、销售、广告、算法等各个团队的数据需求。本文主要介绍美团外卖离线数仓的历史发展历程，在发展过程中碰到的痛点问题，以及针对痛点做的一系列优化解决方案。

**01**

业务介绍

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image2.jpeg)


首先介绍下美团外卖的业务场景，
核心交易链路为：用户可以通过美团的各种用户终端（包括美团外卖的 APP
或者美团
APP、QQ/微信等）下单，然后商家接单、骑手配送，三个阶段完成一笔交易。这一系列交易过程，由包括用户端、商家端、配送平台、数据组、广告组等各个系统协同完成。

这里主要介绍外卖数据组在整个业务中角色。外卖数据组主要是：

-   给用户端、商家端提供业务需求，

-   给前端提供需要展示的数据，

-   给广告、算法团队提供特征等数据，提高算法效率

-   向城市团队提供业务开展所需数据

-   前端提供埋点指标、埋点规范相关数据

### 整体架构介绍

美团外卖整体分为四层：数据源层、数据加工层、数据服务层、数据应用层。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image3.jpeg)


数据源层：包含接入的原始数据，包括客户端日志、服务端日志、业务库、集团数据、外部数据等。

数据加工层：使用 Spark、Hive 构建离线数仓、使用 Storm、 Flink
实时数仓。在数仓之上针对服务对象建设各种数据集市，比如：

-   面向总部使用的总部数据集市

-   面向行为数据的流量数据集市

-   面向线下城市团队的城市团队集市

-   面向广告的广告集市

-   面向算法的算法特征

数据服务层：主要包括存储介质的使用和数据服务的方式。

-   存储：主要使用开源组件，如 Mysql, HDFS, HBase, Kylin, Doris, Druid,
    ES, Tair 等

-   数据服务：对外数据查询、接口以及报表服务

数据应用层：主要包括主题报表、自助取数工具、增值产品、数据分析等支撑业务开展，同时依赖公司平台提供的一些工具建设整体数据应用。

2.  **ETL on Spark**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image4.jpeg)


我们离线计算从 17 年开始从 Hive 迁移到 Spark，
目前大部分任务已经迁移到 Spark 上运行，任务迁移后，相比之前使用 Hive
整体资源节省超过 20%。相比之下 Spark 的主要优势是：

-   算子丰富，支持更复杂的业务逻辑

-   迭代计算，中间结果可以存内存，相比 MR 充分利用了内存，提高计算效率

-   资源复用，申请资源后重复利用

这里简单介绍写 Spark Sql 的任务解析流程：客户端提交任务后，通过 Sql
解析先生成语法树，然后从 Catalog
获取元数据信息，通过分析得到逻辑执行计划，进行优化器规则进行逻辑执行的优化，最后转成物理执行计划提交到
spark集群执行。

### 数仓建设

1.  数据仓库 **V1.0**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image5.jpeg)


2016
年之前。外卖数据组的情况是：团队不大，数据量不多，但是市场竞品较多（饿了么、百度等），竞争激烈，
因此当时数据组的目标是：快速响应业务需求，同时做到灵活多变，支撑业务业务决策，基于这种业务背景和实现目标，当时数仓架构设计如图所示主要分了四层，分别是：ODS
层/明细层/聚合层/主题层/应用层（具体如图示）。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image6.jpeg)


随着数据量、业务复杂度与团队规模的增长，
为更好完成业务需求数据组团队按照应用做拆分，比如面向总部的总部团队、面向城市业务的城市团队，各个团队都做一份自己的明细数据、指标和主题宽表数据，指标和主题宽表很多出现重叠的情况，这时候就像是"烟囱式"开发。这在团队规模较小时，大家相互了解对方做的事情，基本不会有问题；但是在团队规模增长到比较大的时候，多团队"烟囱式"独立作战也暴露出了这种架构的问题，主要是：

-   开发效率低

-   数据口径不统一

-   资源成本高

2.  数据仓库 **V2.0**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image7.jpeg)


针对上述问题，数据组做了架构的升级，就是数据仓库 V2.0
版本。此次升级优化的目标主要是：

-   简化开发流程，提高开发运维效率

-   明确分层，分主题标准，贯彻执行

完成这个目标的思路如下三个方面：

① 分工标准：

之前面向不同应用建立不同团队完全纵向切分，会导致可以公用的部分明细数据重复开发。为改变这种情况将数据团队改为：数据应用组和数据建模组。各组职责如下：

-   数据应用组：负责应用指标、应用维度、应用模型，这一组的数据建模特点是：自上而下、面向应用。

-   数据建模组：负责基础事实、基础维度、原子指标的数据开发，这一组的数据建模特点是：自下而上、面向业务。

② 分层标准：

在原有分层的基础上，再次明确各层的职责，比如：明细层用来还原业务过程，轻度汇总曾用来识别分析对象；同时数据加工时考虑数据的共性、个性、时效性、稳定性四个方面的因素，基于以上原则明确数仓各层达到数据本身和应用需求的解耦的目标。具体各层细节在文章接下来的内容会展开来讲。

③ 主题标准：

根据数仓每层的特性使用不同的主题划分方式，总体原则是：主题内部高内聚、不同主题间低耦合。主要有：明细层按照业务过程划分主题，汇总层按照"实体+活动"划分不同分析主题，应用层根据应用需求划分不同应用主题。

1.  数仓规范

① 数据仓库建模规范

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image8.jpeg)


根据前述三个方面的思路，将数仓分为以下几个层次：

-   ODS：数据源层，主要职责是接入数据源，并做多数据源的整合

-   IDL：数据集成层，主要职责是：业务主题的划分、数据规范化，比如商家、交易、用户等多个主题。这一层主要起到缓冲的作用，屏蔽底层影响，尽量还原业务，统一标准。

-   CDL：数据组件层，主要职责是划分分析主题、建设各个主题的基础指标，比如商家交易、用户活动等多个主题。这样针对同一个分析对象统一了指标口径，同时避免重复计算。

-   MDL：数据集市层，主要职责是建设宽表模型、汇总表模型，比如商家宽表、商家时段汇总表等。主要作用是支撑数据分析查询以及支持应用所需数据。

-   ADL：数据应用层，主要职责是建设应用分析、支撑多维分析应用，比如城市经营分析等。

其中 ODS/IDL/CDL，以及部分 MDL
集市由数据基建组来做，另外部分数据集市以及 ADL
应用层由数据应用组支撑，分工标准是涉及一些公共的数据集市由数据基建组来完成；数据应用组会围绕应用建设应用数据集市，如流量集市、城市经营集市。

② 数据源层

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image9.jpeg)


整体建设思路：从数据源落地到 Hive
表，同时与数据来源保持一致，尽量还原业务。主要由四类数据源：业务库数据、流量日志、集团数据、三方数据。

-   业务库数据：主要是将各个客户端的数据同步 Hive
    ，主要有用户端、商家端、运营端，同步方法主要采用 binlog
    同步，同步方式有全量同步、增量、快照同步三种方式。同时支持业务库分库分表、分集群等不同部署方式下的数据同步。

-   流量日志：特点是外卖终端多，埋点质量不一，比如单 C
    端分类就超过十种。为此公司统一了终端埋点
    SDK，保证不同终端埋点上报的数据规范一致，同时使用一些配置工具、测试工具、监控工具保证埋点的质量。整理建设思路是：定义埋点规范、梳理埋点流程、完善埋点工具。

-   集团数据：包含集团业务数据、集团公共数据，特点是数据安全要求高。目前公司建立了统一的安全仓，用于存储跨
    BU的数据，同时定义权限申请流程。这样对于需要接入的数据，直接走权限申请流程申请数据然后导入业务数仓即可。

-   三方数据：外部渠道数据，特点是外部渠道多、数据格式不统一，对此我们提供了通用接口对于收集或者采买的三方数据在接入数仓前进行了规范化的清洗。

③ 数据集成层

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image10.jpeg)


数据集成层主要是明细数据，与上一层数据源层是有对应关系的。数据集成表的整体建设思路为：

-   抽象业务过程

-   识别实体关系，挂靠业务主题，比如交易过程包括提单、支付等过程，把这些业务行为涉及的事实表进行关联，识别出里面的实体关系

-   兼容数据成本

-   屏蔽业务变化，比如订单状态变化

-   统一数据标准，敏感字段脱敏，字段名称标准化等

如图中示例为提单表建设过程。

④ 数据组件层

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image11.jpeg)


数据组件层，主要建设多维明细模型、轻度汇总模型。总体建设思路与建设原则为：

建设思路：

-   识别分析对象，包含分析对象实体以及对象行为

-   圈定分析边界

-   丰富对象属性

建设原则：

数据组件层生成的指标主要是原子指标，原子指标形成数据组件，方便下游的集市层以及应用层拼接数据表。

-   分析对象包括业务实体和业务行为

-   分析对象的原子指标和属性的惟一封装

-   为下一层提供可共享和复用的组件

多维明细模型：

以商家信息表建设过程为例：

-   识别分析对象：首先明确分析对象为商家实体，

-   圈定分析边界：多维明细不需要关联实体行为，只需要识别出实体之后圈定商家属性信息作为分析边界；

-   丰富对象属性：提取商家属性信息，比如商家的品类信息、组织结构信息等

以上信息就形成了一个由商家主键和商家多维信息组成的商家实体的多维明细模型。

轻度汇总模型：

以商家交易表假设过程为例：

-   识别分析对象：分析实体是商家，业务行为是交易，分析对象是商家交易

-   圈定分析边界：圈定提交表、商家信息表、订单状态表、会员表作为商家交易的边界

-   丰富对象属性：将城市、组织结构等维度信息冗余进来，丰富维度属性信息

汇总商家粒度、交易额等原子指标最终建立商家交易表。

⑤ 数据集市层

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image12.jpeg)


建设思路：

建立宽表模型和汇总模型。两者区别为宽表模型是唯一主键，基于主键拼接各种信息；汇总模型的主键类型为联合主键，根据公共维度关联生成派生指标，丰富信息。

-   宽表模型：订单宽表为例，建设过程为：选定订单实体作为实体对象，然后圈定订单明细、订单状态、订单活动、订单收购等分析对现象通过订单
    id
    进行关联。这里的宽表模型与数据组件层的多维明细模型的区别在于多维明细模型里的实体对象粒度更细，例如订单宽表中分析对象：订单明细、订单状态、订单活动等都是多维明细模型里的一个个数据组件，这几个数据组件通过订单
    id 关联拼接形成了宽表模型。

-   汇总模型：商家时段汇总表为例，建设过程为：选定商家、时段维度作为维度组合，圈定商家和时段维度相关的表，通过公共维度进行关联、维度冗余，支持派生指标、计算指标

的建设。这里区别于组件层的轻度汇总模型，在数据组件层建设的是原子指标，而数据集市层建设的是派生指标。

⑥ 数据应用层

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image13.jpeg)


建设思路：根据应用场景选择合适的查询引擎

选型考虑因素：OLAP 引擎选型考虑以下 8 个方面的因素：

-   数据规模是否适合

-   SQL 语法的支持程度如何

-   查询速度怎么样

-   是否支持明细数据

-   是否支持高并发

-   是否支持数据检索

-   是否支持精确去重

-   是否方便使用，开发效率如何

技术选型：早期主要使用 Kylin ，近期部分应用开始迁移 Doris。

模型：根据不同 OLAP 引擎使用不同数据模型来支持数据应用。基于 Kylin
引擎会使用星型模型的方式构建数据模型，在 Doris
支持聚合模型，唯一主键以及冗余模型。

2.  数仓 **V2.0** 的缺点

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image14.jpeg)


前面几小节对数仓 2.0 做了详细的介绍，在数仓 2.0
版本的建设过程中我们也遇到了一些问题。前面有提到数据集成层与组件层由数据基建组来统一运维，数据应用层是由数据应用组来运维，这样导致虽然在集成层和组件做了收敛但是在应用层和集市层却产生了膨胀，缺乏管理。

面对这个问题，我们在 2019 年对数仓进行了新的迭代，即数仓
V3.0，下面将对此做详细介绍。

3.  数据仓库 **V3.0**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image15.jpeg)


总体愿景：数仓 3.0
优化思路主要是使用建模工具替代人工开发。建模工具：分为基础的建模工具和应用层建模工具。

-   基础层建模工具：主要是在元数据中心记录维护业务过程、表的关联关系、实体对象、识别的分析对象，基于维护的信息构建数据组件以供应用层和集市层拼接

-   自助查询工具：根据数据组件和用户选取的需要查询的指标维度信息构建逻辑宽表，根据逻辑宽表匹配最佳模型从而生成查询语句将查询出来的数据反馈给用户。同时根据用户查询情况反过来指导建模，告诉我们需要把哪些指标和哪些维度经常会放在一起查询，根据常用的指标、维度组合建设数据组件

-   应用层建模工具：依赖数据组件，包括数据组件层的多维明细数据、轻度汇总数据以及集市层的宽表等构建构建数据应用。主要过程是获取所需数据组件，进行数据裁剪，与维表

关联后冗余维度属性，按需进行上卷聚合、复合指标的计算，最终把获取到的多个小模型拼接起来构建数据应用

通过整套工具的使得数据组件越来越完善，应用建模越来越简单，以上就是数仓

3.0 的整体思路。

### 数据治理

1.  数据开发流程

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image16.jpeg)


先说下我们数据开发流程，数据开发流程主要分为四个阶段：需求分析、技术方案设计、数据开发、报表开发&接口开发，具体内容如下：

-   需求分析：在需求分析阶段，产品会设计一个需求 PRD
    以及列出指标维度矩阵，然后需求评审与需求相关人员进行沟通，做一些数据的探查

-   技术方案设计：完成需求分析之后，形成模型设计的技术方案，同时将方案落地到文档进行技术方案的评审

-   数据开发：完成技术方案的设计与评审就正式进入了数据开发以及测试阶段

-   报表开发&接口开发：数据开发完成之后进入具体应用的开发

在整个数据开发流程中，我们遵循的整体思路是：

-   数据标准化

-   标准系统化

-   系统一体化

即数据符合标准规范，同时将标准规范落地到系统里，最后系统要和周边应用打通，形成一体。下面对各个思路做详细的描述。

① 数据标准化

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image17.jpeg)


在数据标准化这块，数据产品团队、数据开发团队、数据分析团队联合建立了数据标准化委员会。数据标准委员会制定了《指标标准规范》、《维度标准规范》以及一些新增指标、维度的流程等一系列规范标准，这样做的好处是：指标维度管理有据可依，指标维度管理有组织保障，保障各业务方指标维度口径清晰统一。

② 标准系统化

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image18.jpeg)


明确了数据标准各项规范，需要将这些标准化规范落地到系统，就是我们的数据治理平台，我们的数据治理平台由自建系统 +
集团数据服务构成。这里面主要有四层：数据生产工具、集团基础平台、元数据层、数据服务层。

-   数据生产工具：数据生产主要依靠平台的计算能力，包括离线生产平台、实时生产平台、调度管理平台

-   集团基础平台：数据生产工具之上是集团基础平台，包括数据资产管理、元数据管理、数据质量管理、资源管理以及权限管理

-   元数据层：元数据与数据服务都是美团外卖自己业务做的一些工作，元数据层包括数据模型、表/字段、主题/层级、指标

/维度、业务过程、词根/词库

-   数据服务层：服务层包含有数据标准化，前面提到的指标流程、维度流程、认证管理都是在这里落地；同时把业务管理起来，包括理业务大盘、业务过程、数据域管理；然后还管

理我们的数据模型包括指标维度矩阵、事实逻辑模型、维度逻辑模型；同时提供元数据服务，包含业务元数据、技术元数据以及维度服务；还有刚才提到的一些在线建模工具

图片右边展示了我们的元数据模型，从下而上，我们首先维护词根组成的词库，同时词根、词库组成我们的指标和维度，其中维度分为维表和码表，指标在确保唯一性的前提下划分业务过程，同时区分原子指标、派生指标、计算指标；然后由维度和指标拼接成字段、字段组成表，表再和业务主题、业务过程相关联，识别出实体、行为，区分事实表、维度表同时确定表所在的层级，最后由一张张的表组成我们的数据模型，整个过程就是我们的元数据模型。

③ 系统一体化

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image19.jpeg)


有了前面的数据信息之后，我们和下游对接就比较方便。使用到数据治理平台的数据下游有：

-   报表系统：报表展示所需的指标维度信息、模型信息都是来自于数据治理平台，

-   数据超市：数据超市的自助取数里根据指标、维度、数据组件构建数据查询，这些信息都是来自数据治理平台，

-   海豚数据平台：海豚数据平台是我们的外卖数据门户

-   异常分析平台：对指标维度的波动进行监测分析

-   CRM 系统：面向一线城市团队的数据展示

-   算法平台：向算法平台提供标签、特征数据的管理

-   画像平台：管理画像平台的人群标签

-   数据 API 服务：对外提供的 API 模型以及接口的信息

-   到家数据检索：到家业务元数据联通，统一检索元数据信息

-   公司元数据平台：向公司元数据平台提供元数据信息

通过与各个下游不同形式的对接，数据治理平台完成了整个下游数据应用的联通，以及支持数据使用与生产，形成了一体化的系统。

2.  资源优化

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image20.jpeg)


资源优化方面，在美团会把核算单元分成若干个租户
，然后把资源分配给各个租户，在同一个租户里各个项目组协调分割分配到资源，项目组由任务和数据组成。

我们把租户与对应主题进行挂靠，这样就可让租户有对应的接口人管理，比如把外卖核算单元分为：数仓租户、广告租户、算法租户以及其他的业务租户，让每个租户对应一个接口人，与接口人对接资源优化方案、规则，最后由接口人推动。

我们的优化规则主要分为三个方向：

-   流量：流量方面，主要是对无效 ODS
    下线从而降低存储与传输成本；同时埋点数据生命周期减少成本浪费，目前用户日活几千万，上报的流量日志是有上百亿，埋点若不再使用对存储与传输成本浪费较大；另外对日志进行序列化压缩处理降低成本

-   存储：存储方面我们使用 ORC
    进行压缩，同时对冷热数据做了生命周期管理，另外也通过模型的优化以及文件的优化把存储成本控制住

-   计算：计算方面对无效任务进行下线、 ETL
    任务优化；同时对数据开发收敛，把业务中公共部分收敛到数据组

有了优化规则，针对规则的运营监控流程为：首先对账单分析，账单主要有离线

/实时计算资源、存储资源、ODS
数据收集使用的资源、日志中心所使用的资源，分析帐单后定义运营规则并将规则落地到数据运维平台，由数据运维平台将任务推送到相关责任人，责任人收到通知后，在数据资产中心做相关处理，同时数据运维平台会做成本监控，对超出配额&预算异常进行报警。

这样我们通过建立统一规则并将规则分发到不同租户落地执行，完成数据资源优化的目标。

3.  数据安全

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image21.jpeg)


数据安全方面主要是对数据脱敏，数据保密等级的设定
(C1\~C4)，数据申请做权限控制，审计数据使用的方式，我们分三个阶段完成数据安全的治理：

-   事前：包括敏感数据脱敏、数据权限控制。针对事业部内、事业部外使用不同的权限流程控制。

-   事中：包括敏感 SQL 的预警与拦截，针对敏感 SQL
    我们进行拦截并由数据安全人员进行审批

-   事后：包括敏感 SQL 审计，操作异常审计。输出敏感
    SQL审计的月报发到对应的部门负责人，审核内容主要有敏感 SQL
    的查询、数据操作异常及后续审批还有全量查询日志分析。

### 未来规划

1.  未来规划思考

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image22.jpeg)


最后介绍下，我们对未来的规划，对未来规划的思考主要是在业务和技术两个方面：

-   业务目标：通过数据赋能业务，帮助完成未来实现业务订单量和收入的高速增长的目标

-   技术目标：提高高效、易用、高质量、低成本的数据服务

这里面数据价值的具体体现，总结为以下几点：

-   基础的数据能力：保障数据服务的稳定性，以及数据的时效性越来越高，以及数据服务的覆盖度足够广、足够全，扩大数据服务内容

-   运营决策支持：及时洞察业务变化，直到业务完成运营决策

-   数据商业变现：通过增值型数据产品，把数据进行商业变现

-   业务数据支持：更好的支持对接的各个业务系统，从而提高整体数据价值

-   算法效率提升：针对算法加工特征所需的数据提供更好的支持，以提高算法模型效率，完成用户转化率的提高

-   社会影响力：通过我们的 PR 团队做一些对外的分析报告，扩大行业影响力

2.  未来规划实施

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image23.jpeg)


针对对未来规划的思考，我们具体实施措施计划是：与集团基础平台工具共享共建，在数据应用方面更好做到数据赋能业务，然后就是具体的数据建设、数据管理。

数据建设：

数据建设主要围绕以下几个方面：

-   数据全：我们希望我们的数据足够全，包括外卖的数据以及团购、点评的线下数据和外部采买的数据等，只要是外卖需要的数据我们都尽量采集过来

-   效率高：效率提升方面，我们刚刚提到我们的使用建模工具替代人工工作从而提高我们的效率

-   能力强：在足够全的数据、提升效率的基础上提高我们的能力，包括服务的稳定性、数据质量

数据管理：

通过完善数据标准规范，并将规范落地到工具以及增强数据治理，另外通过算法
的手段发现数据里隐藏的问题完成数据数据治理。这主要需要我们组织能力建设、标准规范的统一、完善数据治理平台与数据运营机制、探索智能数据治理，最终达到数据管理的规范化、系统化、智能化的目的。



![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image24.jpeg)


### 二、OneData 建设探索之路：SaaS 收银运营数仓建设

## 背景

-   缺乏统一的业务和技术标准，如：开发规范、指标口径和交付标准不统一。

## 目标

## OneData 探索

#### OneData：行业经验

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image25.png)


图 1 OneData 标准

#### OneData：我们的思考

OneData：我们的想法

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image27.jpeg)


图 2 OneData 的六个特性

OneData：我们的策略

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image29.png)


## OneData 实践

#### 统一业务归口

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image31.png)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image32.png)


图 3 支持业务的数据源知识库

#### 设计统一归口

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image35.jpeg)


图 4 数据分层架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image37.jpeg)


图 5 重构前和重构后的数据流向图

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image39.jpeg)


图 6 统一的表命名规范

<table style="width:100%;">
<colgroup>
<col style="width: 1%" />
<col style="width: 19%" />
<col style="width: 11%" />
<col style="width: 17%" />
<col style="width: 20%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 6%" />
<col style="width: 1%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th colspan="8"><p><strong>(3) 指标命名规范</strong></p>
<p>结合指标的特性以及词根管理规范，将指标进行结构化处理。</p>
<p>A. 基础指标词根，即所有指标必须包含以下基础词根：</p></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="10"><blockquote>
<p><strong>英 文 全 Hive 数据类 MySQL 数据类 长 精 词 样</strong></p>
<p><strong>基础指标词根</strong></p>
<p><strong>称 型 型 度 度 根 例</strong></p>
</blockquote></td>
</tr>
<tr class="even">
<td colspan="2"><blockquote>
<p>数量 金额类</p>
<p>比率/占比</p>
<p>……</p>
</blockquote></td>
<td><blockquote>
<p>count amout ratio</p>
<p>……</p>
</blockquote></td>
<td><blockquote>
<p>Bigint Decimal Decimal</p>
<p>……</p>
</blockquote></td>
<td><blockquote>
<p>Bigint Decimal Decimal</p>
<p>……</p>
</blockquote></td>
<td><blockquote>
<p>10</p>
<p>20</p>
<p>10</p>
</blockquote></td>
<td><blockquote>
<p>0</p>
<p>4</p>
<p>4</p>
</blockquote></td>
<td><blockquote>
<p>cnt amt ratio</p>
<p>……</p>
</blockquote></td>
<td colspan="2"><blockquote>
<p>0.9818</p>
</blockquote></td>
</tr>
</tbody>
</table>

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image41.png)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image43.jpeg)
图 7 普通指标规范

图 8 日期类型指标规范

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image45.jpeg)


图 9 聚合类指标规范


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image47.jpeg)


图 10 模型设计和审计职责

#### 统一应用归口

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image49.jpeg)


图 11 应用归口

#### 统一数据出口

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image51.jpeg)


图 12 交付标准化

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image53.png)


图 13 起源功能体系

## 实践的成果

#### 流程改善

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image55.png)


图 14 数仓管理流程

#### 数仓全景图

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image57.png)


图 15 数仓全景图

#### 资产管理列表

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image59.jpeg)


图 16 数据资产管理

#### 项目收益

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image61.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image62.jpeg)


图 17 价值收益

## 总结和展望

### 三、美团点评酒旅数据仓库建设实践

## 技术架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image65.png)


Hotel dw layer

我们把它们简称为三代数仓模型层次。在第一代数仓模型层次中，由于当时美团整体的业务系统所支持的产品形式比较单一（团购），业务系统中包含了所有业务品类的数据，所以由平台的角色来加工数据仓库基础层是非常合适的，平台统一建设，支持各个业务线使用，所以在本阶段中我们酒旅只是建立了一个相对比较简单的数据集市。

但随着美团原本集中的业务系统不能快速响应各个业务线迅速的发展与业务变
化时，酒旅中的酒店业务线开始有了自己的业务系统来支持预订、房惠、团购、直连等产品形式，境内度假业务线也开始有了自己的业务系统来支持门票预订、门票直连、跟团游等复杂业务。我们开始了第二代数仓模型层次的建设，由建设数据集市的形式转变成了直接建设酒旅数据仓库，成为了酒旅自身业务系统数据的唯一加工者。由于系统调整初期给我们带来的重构、修改以及新增等数据处理工作非常大，我们采用了比较短平快的
Kimball 所提的维度建模的方式建设了酒旅数据仓库。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image67.png)


Hotel dw baslayer background

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image69.png)


baslayer adventage

## 业务架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image71.png)


dw topic

参与人主题

流量主题

订单主题

POI 主题

产品主题

运营主题

结算主题

## 整体架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image73.png)


dw architecture

#### 订单主题

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image75.png)


dw architecture

#### 流量主题

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image77.png)


dw architecture

#### 运营主题

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image79.png)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image81.png)
dw architecture

dw architecture



![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image24.jpeg)


# 实时数仓

### 四、美团外卖实时数仓建设实践

## 实时场景

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image83.jpeg)


2.  ## 实时技术及架构

    1.  #### 实时计算技术选型

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image85.jpeg)


2.  实时架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image87.png)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image89.jpeg)
Lambda
是比较经典的一款架构，以前实时的场景不是很多，以离线为主，当附加了实时场景后，由于离线和实时的时效性不同，导致技术生态是不一样的。而
Lambda
架构相当于附加了一条实时生产链路，在应用层面进行一个整合，双路生产，各自独立。在业务应用中，顺理成章成为了一种被采用的方式。

双路生产会存在一些问题，比如加工逻辑 Double，开发运维也会
Double，资源同样会变成两个资源链路。因为存在以上问题，所以又演进了一个
Kappa 架构。

##### ② Kappa 架构

Kappa
从架构设计来讲，比较简单，生产统一，一套逻辑同时生产离线和实时。但是在实际应用场景有比较大的局限性，在业内直接用
Kappa
架构生产落地的案例不多见，且场景比较单一。这些问题在美团外卖这边同样会遇到，我们也会有自己的一些思考，将会在后面的章节进行阐述。

## 业务痛点

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image91.jpeg)
首先，在外卖业务上，我们遇到了一些问题和挑战。在业务早期，为了满足业务需要，一般是
Case By Case
地先把需求完成。业务对于实时性要求是比较高的，从时效性的维度来说，没有进行中间层沉淀的机会。在这种场景下，一般是拿到业务逻辑直接嵌入，这是能想到的简单有效的方法，在业务发展初期这种开发模式也比较常见。

如上图所示，拿到数据源后，我们会经过数据清洗、扩维，通过 Storm 或
Flink进行业务逻辑处理，最后直接进行业务输出。把这个环节拆开来看，数据源端会重复引用相同的数据源，后面进行清洗、过滤、扩维等操作，都要重复做一遍。唯一不同的是业务的代码逻辑是不一样的，如果业务较少，这种模式还可以接受，但当后续业务量上去后，会出现谁开发谁运维的情况，维护工作量会越来越大，作业无法形成统一管理。而且所有人都在申请资源，导致资源成本急速膨胀，资源不能集约有效利用，因此要思考如何从整体来进行实时数据的建设。

## 数据特点与应用场景

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image93.jpeg)


5.  ## 实时数仓架构设计

    1.  #### 实时架构：流批结合的探索

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image95.jpeg)
基于以上问题，我们有自己的思考。通过流批结合的方式来应对不同的业务场景。

如上图所示，数据从日志统一采集到消息队列，再到数据流的 ETL
过程，作为基础数据流的建设是统一的。之后对于日志类实时特征，实时大屏类应用走实时流计算。对于
Binlog 类业务分析走实时 OLAP 批处理。

流式处理分析业务的痛点是什么？对于范式业务，Storm 和 Flink
都需要很大的外存，来实现数据流之间的业务对齐，需要大量的计算资源。且由于外存的限制，必须进行窗口的限定策略，最终可能放弃一些数据。计算之后，一般是存到
Redis里做查询支撑，且 KV 存储在应对分析类查询场景中也有较多局限。

#### 实时数仓架构设计

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image97.jpeg)
总结起来，从整个实时数仓的建设角度来讲，首先数据建设的层次化要先建出来，先搭框架，然后定规范，每一层加工到什么程度，每一层用什么样的方式，当规范定义出来后，便于在生产上进行标准化的加工。由于要保证时效性，设计的时候，层次不能太多，对于实时性要求比较高的场景，基本可以走上图左侧的数据流，对于批量处理的需求，可以从实时明细层导入到实时
OLAP 引擎里，基于 OLAP
引擎自身的计算和查询能力进行快速的回撤计算，如上图右侧的数据流。

## 实时平台化建设

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image99.jpeg)
首先进行功能的抽象，把功能抽象成组件，这样就可以达到标准化的生产，系统化的保障就可以更深入的建设，对于基础加工层的清洗、过滤、合流、扩维、转换、加密、筛选等功能都可以抽象出来，基础层通过这种组件化的方式构建直接可用的数据结果流。这会产生一个问题，用户的需求多样，为了满足了这个用户，如何兼容其他的用户，因此可能会出现冗余加工的情况。从存储的维度来讲，实时数据不存历史，不会消耗过多的存储，这种冗余是可以接受的，通过冗余的方式可以提高生产效率，是一种**以空间换时间**思想的应用。

通过基础层的加工，数据全部沉淀到 IDL 层，同时写到 OLAP
引擎的基础层，再往上是实时汇总层计算，基于 Storm、Flink 或
Doris，生产多维度的汇总指标，形成统一的汇总层，进行统一的存储分发。

当这些功能都有了以后，元数据管理，指标管理，数据安全性、SLA、数据质量等系统能力也会逐渐构建起来。

#### 实时基础层功能

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image101.jpeg)
实时基础层的建设要解决一些问题。首先是一条流重复读的问题，一条
Binlog打过来，是以 DB
包的形式存在的，用户可能只用其中一张表，如果大家都要用，

可能存在所有人都要接这个流的问题。解决方案是可以按照不同的业务解构出来，还原到基础数据流层，根据业务的需要做成范式结构，按照数仓的建模方式进行
集成化的主题建设。

其次要进行组件的封装，比如基础层的清洗、过滤、扩维等功能，通过一个很简单的表达入口，让用户将逻辑写出来。数据转换环节是比较灵活的，比如从一个值转换成另外一个值，对于这种自定义逻辑表达，我们也开放了自定义组件，可以通过
Java 或 Python 开发自定义脚本，进行数据加工。

#### ![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image103.jpeg)实时特征生产功能

特征生产可以通过 SQL
语法进行逻辑表达，底层进行逻辑的适配，透传到计算引擎，屏蔽用户对计算引擎的依赖。就像对于离线场景，目前大公司很少通过代码的方式开发，除非一些特别的
Case，所以基本上可以通过 SQL 化的方式表达。

在功能层面，把指标管理的思想融合进去，原子指标、派生指标，标准计算口径，维度选择，窗口设置等操作都可以通过配置化的方式，这样可以统一解析生产逻辑，进行统一封装。

还有一个问题，同一个源，写了很多
SQL，每一次提交都会起一个数据流，比较浪费资源，我们的解决方案是，通过同一条流实现动态指标的生产，在不停服务的情况下可以动态添加指标。

所以在实时平台建设过程中，更多考虑的是如何更有效的利用资源，在哪些环节更能节约化的使用资源，这是在工程方面更多考虑的事情。

#### SLA 建设

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image105.jpeg)


4.  实时OLAP 方案

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image107.jpeg)
通过带计算能力的 OLAP
引擎来解决，不需要把一个流进行逻辑化映射，只需要解决数据实时稳定的入库问题。

我们这边采用的是 Doris 作为高性能的 OLAP
引擎，由于业务数据产生的结果和结果之间还需要进行衍生计算，Doris
可以利用 Unique
模型或聚合模型快速还原业务，还原业务的同时还可以进行汇总层的聚合，也是为了复用而设计。应用层可以是物理的，也可以是逻辑化视图。

这种模式重在解决业务回撤计算，比如业务状态改变，需要在历史的某个点将值变更，这种场景用流计算的成本非常大，OLAP
模式可以很好的解决这个问题。

## 实时应用案例

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image109.jpeg)


### 五、美团基于 Flink 的实时数仓建设实践

## 引言

## 实时平台初期架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image111.jpeg)


图 1 初期实时数据架构

## 实时数据仓库的构建

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image113.jpeg)


图 2 实时数仓数据分层架构

技术选型

1.  **存储引擎的调研**

| **方案**       | **优势**                                                                 | **劣势**                                                                 |
|----------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|
| MySQL          | 1. 具有完备的事务功能，可以对数据进行更新。2. 支持SQL，开发成本低。       | 1. 横向扩展成本大，存储容易成为瓶颈；2. 实时数据的更新和查询频率都很高，线上单个实时应用请求就有1000+ QPS；使用MySQL成本太高。 |
| Elasticsearch  | 1. 吞吐量大，单个机器可以支持2500+ QPS，并且集群可以快速横向扩展。2. Term查询时响应速度很快，单个机器在2000+ QPS时，查询延迟在20 ms以内。 | 1. 没有原生的SQL支持，查询DSL有一定的学习门槛；2. 进行聚合运算时性能下降明显。 |
| Druid          | 1. 支持超大数据量，通过Kafka获取实时数据时，单个作业可支持6W+ QPS；2. 可以在数据导入时通过预计算对数据进行汇总，减少的数据存储，提高了实际处理数据的效率；3. 有很多开源OLAP分析框架，实现如Superset。 | 1. 预聚合导致无法支持明细的查询；2. 无法支持Join操作；3. Append-only不支持数据的修改，只能以Segment为单位进行替换。 |
| Cellar         | 1. 支持超大数据量，采用内存加分布式存储的架构，存储性价比很高；2. 吞吐性能好，经测试处理3W+ QPS读写请求时，平均延迟在1ms左右；通过异步读写线上最高支持10W+ QPS。 | 1. 接口仅支持KV、Map、List以及原子加减等；2. 单个Key值不得超过1KB，而Value的值超过100KB时则性能下降明显。 |



![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image115.jpeg)


图 3 实时数仓存储分层架构

2.  **计算引擎的调研**

| **项目/引擎**       | **Storm**                                                                 | **Flink**                                                                 | **Spark-Streaming**                                                               |
|--------------------|---------------------------------------------------------------------------|---------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| API                | 灵活的底层API和具有事务保证的Trident API                                  | 流API和更加适合数据开发的Table API、Flink SQL支持                           | 流API和Structured-Streaming API，同时也可以使用更适合数据开发的Spark SQL          |
| 容错机制           | ACK机制                                                                   | State分布式快照保存点                                                     | RDD保存点                                                                        |
| 状态管理           | Trident State状态管理                                                     | Key State和Operator State两种State可以使用，支持多种持久化方案               | 有UpdateStateByKey等API进行带状态的变更，支持多种持久化方案                      |
| 处理模式           | 单条流式处理                                                             | 单条流式处理                                                             | Mic batch处理                                                                    |
| 延迟               | 毫秒级                                                                   | 毫秒级                                                                   | 秒级                                                                            |
| 语义保障           | At Least Once，Exactly Once                                              | Exactly Once，At Least Once                                              | At Least Once                                                                    |


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image117.jpeg)


图 4 Flink - Storm 对比图

Flink 使用心得

1.  **维度扩充**

2.  **数据关联**

3.  **聚合运算**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image119.jpeg)


图 5 实时计算流程图

#### 实时数仓成果

进行开发，代码加简洁。单个作业的代码量从平均 300+ 行的 JAVA 代码
，缩减到几十行的 SQL
脚本。项目的开发时长也大幅减短，一人日开发多个实时数据指标情况也不少见。

除此以外我们通过针对数仓各层级工作内容的不同特点，可以进行针对性的性能
优化和参数配置。比如 ODS 层主要进行数据的解析、过滤等操作，不需要 RPC
调用和聚合运算。 我们针对数据解析过程进行优化，减少不必要的 JSON
字段解析，并使用更高效的 JSON 包。在资源分配上，单个 CPU 只配置 1GB
的内存即可满需求。而汇总层主要则主要进行聚合与关联运算，可以通过优化聚合算法、内外存共同运算来提高性能、减少成本。资源配置上也会分配更多的内存，避免内存溢出。通过这些优化手段，虽然相比原有流程实时数仓的生产链路更长，但数据延迟并没有明显增加。同时实时数据应用所使用的计算资源也有明显减少。

## 展望

# 数据平台

# 

### 六、美团数据平台融合实践

互联网格局复杂多变，大规模的企业合并重组不时发生。原来完全独立甚至相互竞争的两家公司，有着独立的技术体系、平台和团队，如何整合，技术和管理上的难度都很大。2015
年 10
月，美团与大众点评合并为今天的"美团"，成为全球规模最大的生活服务平台。主要分布在北京和上海两地的两支技术团队和两套技术平台，为业界提供了一个很好的整合案例。

本文将重点讲述数据平台融合项目的实践思路和经验，并深入地讨论
Hadoop多机房架构的一种实现方案，以及大面积 SQL
任务重构的一种平滑化方法。最后介绍这种复杂的平台系统如何保证平稳平滑地融合。

两家公司融合之后，从业务层面上，公司希望能做到"1+1\>2"，所以决定将美团和大众点评两个
App
的入口同时保留，分别做出各自的特色，但业务要跨团队划分，形成真正的合力。比如丽人、亲子、结婚和休闲娱乐等综合业务以及广告、评价
UGC
等，都集中到上海团队；而餐饮、酒店旅游等业务集中到北京团队。为了支撑这种整合，后台服务和底层平台也必须相应融合。

点评 App 和美团 App
的数据，原来会分别打到上海和北京两地的机房，业务整合之后，数据的生产地和数据分析的使用地可能是不一样的。同时，随着公司的融合，我们跨团队、跨业务线的分析会越来越多，并且还需要一些常态化的集团级报表，包括流量的分析表、交易的数据表，而这些在原来都是独立的。

举个例子，原点评侧的分析师想要分析最近一年访问过美团和大众点评两个App的重合用户数，他需要经过这样一系列的过程：如下图所示，首先他要想办法找

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image121.jpeg)


1

## 确立目标

## 难点

#### 架构复杂，基础设施限制

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image123.jpeg)


2

#### 可靠性要求

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image125.jpeg)
由于团购网站竞争激烈，两家公司对于用数据去优化线上的一些运营策略以控制运营成本，以及用数据指导销售团队的管理与支撑等场景，都有极强的数据驱动意识，管理层对于数据质量的要求是特别高的。我们每天从零点开始进行按天的数据生产，工作日
9
点，老板们就坐在一起去开会，要看到昨天刚刚发生过什么、昨天的运营数据怎么样、昨天的销售数据怎么样、昨天的流量数据怎么样；工作日
10 点，分析师们开始写临时查询，写 SQL 去查数据，包括使用
Presto、Hive，一直到 22
点；同时数据科学家开始去调模型。如果我们集群不能
work，几千人每天的工作就只能坐在电脑面前看着 Excel......

当时的分析是这样，如果考虑回滚的情况下，我们运维的时间窗口在平日只有一个小时，而且要对全公司所有用数据的同学进行通告，这一个小时就是他们下班之后，晚上
6 点至 7 点的时候开始，做一个小时，如果一个小时搞不定，回滚

还有一个小时。周末的话好一点，可以做 4
小时之内，然后做全面的通告，相当于整个周末大家都没法加班了，他们是非常不开心的。

3

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image127.jpeg)


4

体量

平台化与复杂度

## 数据互访打通

#### 原始层数据收集

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image129.png)


5

#### 集群数据互拷

集群数据互拷，也就是
DistCp。这里稍微有一点挑战的是两边的调度系统分别开了接口，去做互相回调。如果我们有一份数据，我想它
ready 之后就立即拷到另外一边，比如原点评侧有个表，我要等它 ready
了之后拷到原美团侧，这个时候我需要在原美团侧这边配一个任务去依赖原点评侧某一个任务的完成，就需要做调度系统的打通。本文主要讨论大数据框架的部分，所以上面的调度系统还有开发平台的部分都是我们工具链团队去做的，就不多说了，下文重点描述
DistCp。

其实 Hadoop 原生支持 DistCp，就是我起一个 MapReduce 在 A
集群，然后并行地去从 B 集群拖数据到 A
集群，就这么简单。只要你网络是通的，账号能认（比如说你在 A
集群跑的任务账号能被 B
集群认），并且有对应的读权限，执行端有计算资源，用开源版本的 DistCp
就可以搞定。

这方面我们做了一些权衡：

首先是因为涉及到带宽把控的问题，所以同步任务是由平台团队来统一管理，业务侧的同学们提需求。

#### Kerberos 跨域认证架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image130.jpeg)


6

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image132.jpeg)


7

## 集群融合

粗看起来，打通了数据平台，我们的大目标似乎已经完成了：一个集群、一套数据平台的工具、一套开发规范。把数据拷过来，然后重新改它的任务，就可以形成在统一的一套工具和规范里面用一个集群，然后慢慢把原来团队维护的服务都下掉就好了。事实上不是这样的，这里面有大量的坑。如果接下来我们什么都不做的话，会发生什么情况呢？

数据 RD
会需要在迁移的目标平台重建数据，比如说我们都定了，以后把原美团侧平台砍掉，那么好，以后都在原点评侧的平台，包括平台的上传工具、平台的集群去使用、去开发。这个时候，至少原美团侧的同学会说："原点评那边平台的那些概念、流程，可能都跟我不一样啊，我还需要有学习的时间，这都还好"。但他们怎么迁移数据呢？只能从源头开始迁移，因为对端什么都没有，所以要先做数据的拷贝，把上游所有的表都拷贝过去。然后一层一层地去改，一整套任务都要完全重新构建一遍。

那我们有多少任务呢？

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image134.jpeg)


8

#### 集群融合的问题本质

集群融合的解决思路

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image136.jpeg)
再一个就是原点评侧的机房确实放不下了，它当时只能扩容到
10 月，再往后扩就装不下机器了。

所以我们将原点评侧的集群，合并到原美团侧机房，然后进行拷贝和切换。我们让整个这个集群变成在原美团侧机房一样的样子，然后进行融合。我们会把上面的客户端和元数据统一，使得访问任何一个集群的时候，都可以用一套客户端来做。一旦我们做到这个样子之后，基于统一的数据、集群的元数据和访问入口之后，我们上面的工具链就可以慢慢地去做一个一个机制，一个一个模块的融合了。

简单总结下来就是四步：统一、拷贝、切换、融合，下面我们来展开说一下这四步。

9

**统一**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image138.jpeg)
我们后面要持续地去折腾 Hadoop
集群，所以必须先把原上海侧的 HDFS 架构改全，改成高可用的。

这里有一个小经验就是，我们自研的 patch 对改的 bug 或者是加的
feature，一定要有一个机制能够管理起来，我们内部是用 Git
去管理的，然后我们自研的

部分会有特殊的标签，能一下拉出来。我们当时其实互相 review 了上百个
patch，因为当时两个团队都有对集群，包括 Hive
等等这些开源软件的修改。这是统一
的阶段，相对容易，就是一个梳理和上线的过程。接下来是拷贝的阶段。

10

**拷贝**

**切换**

这个相当于把原点评侧的
NameNode（这个时候还没有彻底下线）切换到原美团侧机房，然后把对应的
YARN 重新启动起来。这里有一个小 trick
就是原美团侧机房的承载能力，大概是 1000
多台节点，是原点评侧的两倍，所以我们才能做这个事，最近我们刚刚把上海机房的节点迁完。

那整个集群的拷贝和切换是怎么做的呢？其实就是用我们自研的一套
Hadoop多机房架构。可能做 Hadoop
集群维护管理的同学们对这个有深刻的体会，就是不时地就要从一个机房搬到另一个机房。设计目标是说我们一个
Hadoop
集群可以跨机房去部署，然后在块的力度上能控制数据副本的放置策略，甚至是进行主动迁移。

设计是怎么做的呢？整个 Hadoop 原生的架构其实没有机房这个概念，只支持
Rack
也就是机架，所有服务器都被认为是在同一个机房的。这个时候不可避免地就会有很多跨机房的流量，就如果你真的什么都不干，就把
Hadoop
跨机房去部署的话，那么不好意思，你中间有好多的调用和带宽都会往这儿走，最大的瓶颈是中间机房网络带宽的资源受限。

我们梳理了一下跨机房部署的时候大概都有哪些场景会真正引发跨机房流量，基本上就这
3\~4 个。首先是写数据的时候，大家知道会 3 副本，3 个 DataNode去建
pipeline，这个时候由于是机器和机器之间建连接，然后发数据的，如果我

要分机房部署的话，肯定会跨机房。那我要怎么应对呢？我们在
NameNode专门增加 zone 的概念，相当于在 Rack
上面又加了一层概念，简单改了一些代码。然后修改了一下 NameNode
逻辑。当它去建立 pipeline 的时候，在那个调用里面 hack 了一下。建
pipeline 的时候，我只允许你选当前这个 Client 所属的
zone，这样写数据时就不会跨机房了。

这些 Application 在调度的时候有可能会在两个机房上，比如说 mapper 在 A
机房，reducer 在 B 机房，那么中间的带宽会非常大。我们怎么做的呢？在
YARN的队列里面，也增加 zone 的概念，我们用的是 Fair
Scheduler。在队列配置里面，对于每一个叶子队列，都增加了一个 zone
的概念。一个叶子队列，其实就是对应了这个叶子队列下面的所有任务，它在分配资源的时候就只能拿到这个
zone
的节点。读取数据的时候有可能是跨机房的，那这个时候没有办法，我们只有在读取块选择的时候本地优先。我们有一些跨机房提交job
的情况，提交 job的时候会把一些 job
里面的数据进行上传，这个时候加了一些任务的临时文件上传的是任务所在的目标机房。这里做一些简单的改动，最重要的是提供了一个功能，就是我们在拷贝数据的时候，其实用
balancer 所用的那一套接口，我们在此基础之上做了一层
Hack，一层封装。形成了一个工具，我们叫
ZoneTransfer，又由它来按照我们一系列的策略配置去驱动 DataNode
之间的跨机房的 block粒度的拷贝。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image140.jpeg)


11

上图是我们跨机房架构的架构图，下面的 Slave 里面有 DN(DataNode)和
NM(NodeManager)，上面跑的同颜色的是一个 App。我们在
RM(ResourceManager)里面的叶子队列里配置了 zone
的概念，然后在调度的时候如大家所见，一个 App
只会在一个机房。然后下面黑色的线条都是写数据流程，DN 之间建立的
pipeline 也会在一个机房，只有通过 root 去做的，DN之间做数据 transfer
的时候才会跨机房进行，这里我们基本上都卡住了这个跨机房的带宽，它会使用多少都是在我们掌控之内的。

在上线和应用这个多机房架构的时候，我们有一些应用经验。

首先在迁移的过程当中我们需要评估一点就是带宽到底用多少，或者说到底多长时间之内能完成这个整体数据的拷贝。这里需要面对的一个现实就是，我们有很多数据是会被持续更新的。比如我昨天看到这个块还在呢，今天可能由于更新被删，那昨天已经同步过来的数据就白费了。那我昨天已经同步过来的数据就白费

了。所以我们定义了一个概念叫拷贝留存率。经过 4
个月的整体拷贝，拷贝留存率大概是 70%多，也就是说我们只有
70%的带宽是有效的，剩下的 30%拷过去的数据，后面都被删了。

第二个是我们必须得有元数据的分析能力，比如说有一个方法能抓到每一个块，我要拷的块当前分布是什么样子。我们最开始是用
RPC 直接裸抓 Active
NameNode，其实对线上的影响还是蛮大的。后面变成了我们通过 FsImage
去拉文件的列表，形成文件和块的列表，然后再到把请求发到
standby，那边开了一个小口子，允许它去读。因为 FsImage 里面是没有block
在哪一个DataNode的元信息的。

这里需要注意的一点就是，我们每天都会有一个按天的数据生产，为了保证它的一致性，必须在当天完成。在切换之前，让被切换集群的
NN（NameNode）进入 SafeMode
的状态，然后就不允许写了，所有的写请求停止，所有的任务停止。我们当时上线大概花了
5\~6
个小时吧，先停，然后再去拷贝数据，把当天的所有新生产的数据都拷过来，然后再去做操作。这里最基本的要做到一点就是，我们离线的大数据带宽不能跟线上的服务的带宽抢资源，所以一定要跟基础设施团队去商量，让他们做一些基于打标签的带宽隔离策略。

**融合**

## 开发工具融合

## 原点评侧拆库

#### 难点

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image142.jpeg)


12

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image144.jpeg)


20

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image146.jpeg)


13

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image148.jpeg)
下图是我们的原始结构，首先这里有一个大前提是每一个任务只对一个结果表。原始的结构中，a
表只依赖 o1 表，b 表依赖 o1、o2，然后 c 表只依赖
o2，它们之间相互关联。这时候我希望可以对库名和表名进行一次性的修改。那如果我们逐层地去改写怎么办呢？首先要先把最上层的
mart
表改了，而我一旦改上游的某一个表，所有跟对它有依赖的表都必须改任务内容。每推动一层改动，下面一层都要变动多次，这样一来，我们这个流程就非常受限。

14

#### 解决方案

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image150.jpeg)


15

他希望的样子而不受任何影响，他写的那些表还是原来的那些表，真正在物理上的存在还是
bi.什么什么这样的表，我们整个项目就 run 起来了。

具体的实施流程是这样，首先先梳理业务，确定整体的映射关系。然后 Hive
元数据入口上去做别名能力，我们是在 Hive metaserver
里面去改的，大部分请求都在这里面，包括 Spark 的、Presto 的、Hive
的等，都能兼容掉，推动分批次改写，单任务内以及任务链条内完全不需要做依赖关系的约束，最终真正实现的是自动化地把
SQL
文本替换掉了。业务的同学们只需要批量看一个检测报告，比如说数据对应上有没有什么问题，然后一键就切了。

我们用了一个季度业务侧来磨合、尝试练习和熟练，同时做工具的开发。然后第二个季度结束后，我们就完成了
7000 多个任务中 90%SQL
任务批量的改写。当任务都切完了之后，我们还有手段，因为所有的请求都是从
Hive 的 metaserver
去访问的，当你还有原有的访问模式的时候，我就可以找到你，你是哪一个任务来的，然后你什么东西改没改，改完了之后我们可以去进行物理上的真正切分，干掉这种元数据对应关系。

物理上的真正切分其实就是把原来都统一的库，按照配置去散到真实的物理上对应的库上，本质还是改
NN 一个事情。

## 未来------常态化多机房方案

## 反思------技术换运营

## 体会------复杂系统重构与融合

最后稍微聊一下复杂系统的重构与融合。从项目管理的角度上来讲，怎么去管
控？复杂系统的重构还有融合本质上最大的挑战其实是一个复杂度管理的事情，我们不可能不出问题，关键是出问题后，对影响的范围可控。

从两个层面去拆分，第一个层面是，先明确定义目标，这个目标是能拆到一个独立团队里去做的，比如说我们最开始那四个大的目标，这样保证团队间能并行地进行推动，其实是一点流水线的思路。第二，我们在团队内进行目标的拆分，拆分就相对清晰了，先确定我要变更什么，然后内部
brainstorming，翻代码去查找、测试、分析到底会对什么东西产生影响，然后去改动、测试、制定上线计划。

内部要制定明确的上线流程，我记得当时在做的时候从 11 月到 12
月我们拆分了应该是有 11
次上线，基本上每次大的上线都是在周末做的，10、11、12 月总共有 12
个周末，一共上线 11 次，大的上线应该是占了 7 到 8 个周末吧。要

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image152.jpeg)


### 七、研发团队资源成本优化实践

## 背景

工程师主要面对的是技术挑战，更关注技术层面的目标。研发团队的管理者则会把实现项目成果和业务需求作为核心目标。实际项目中，研发团队所需资源（比如物理机器、内存、硬盘、网络带宽等）的成本，很容易被忽略，或者在很晚才考虑。

在一般情况下，如果要满足更多的技术指标如并发量和复杂度等，或者满足峰值业务的压力，最直接有效的方法就是投入更多的资源。然而，从全局来看，如果资源成本缺乏优化，最终会出现如下图所示的"边际效用递减"现象------技术

能力的提升和资源的增幅并不匹配，甚至资源的膨胀速度会超过技术能力的提升，从而使技术项目本身的
ROI 大打折扣。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image154.jpeg)
从笔者十余年的工作经验来看，资源成本优化宜早不宜迟。很多管理者在业务发展的较前期阶段，可能并没有意识到资源成本膨胀所带来的压力。等到业务到了一定规模，拿到机器账单的时候，惊呼"机器怎么这么费钱"，再想立即降低成本，可能已经错过了最佳时机，因为技术本身是一个相对长期的改造过程。

所以，正在阅读此文的读者，假如你已经感觉到了成本膨胀的压力，或者正在做成本控制相关的工作，恭喜，这是幸福的烦恼，贵公司的业务体量应该已经达到一定规模，同时也说明你的管理意识可能已经超前于业务发展了（握手）。

本文我们将分享美团到餐研发团队的资源成本优化实践。

## 实践

1.  确定方法论

2.  计划规划阶段（Plan&Standard）

3.  执行阶段（Do）

    1.  **建立思考流程**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image156.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image158.png)


2.  **实践分析框架**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image159.jpeg)


建立了以上的结构，就可以根据各个专业的不同，对各自的指标进行优化了，如果最细一级的指标被成功优化之后，最上层的指标一定会有下降。因为上述指标都有其各自深层次的业务、技术，甚至是财务上的逻辑，故在此把一些需要关注的概念再赘述一下.

很多公司每个技术团队的机器成本，在财务上叫做"网站运维成本"（网站？听起来还像
PC
时代的概念对不对），从顶层可以分为两类构成因素，就是"自己产生的成本"（自己用的）和"被分摊的成本"（别人替你用的）两大类。跟自己有关的继续向下钻取，可以分为交易相关的资源成本（跟业务流程相关的）以及跟分析有关的大数据成本（分析、算法、决策相关）。

##### 业务主机成本

大部分业务系统的团队，使用的资源成本都包含在这个部分，比如商户研发团队、订单系统研发团队、前端研发团队、供应链研发团队、营销系统研发团队、CRM研发团队等。这些资源典型的物理载体就是物理机、虚拟机、容器资源以及对应

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image161.jpeg)
的机器连接的存储（DB、缓存、K-V
数据库等）资源，还会包含由于交换、存储以上资源之间的数据产生的带宽、云资源、CDN
等。

这部分资源，我们从控制成本的角度，最浅的层次，建议关注服务组（OWT）所消耗主机的资源利用率，如果资源利用率较低的主机数量较多，建议及时下线。同时，从技术方案本身来说，任何一个服务承载的业务能力和消耗资源之间，会有相对的一个"比例"或者权重。某些高利用率的服务从架构上是否可以重构、解耦或者改造，也非常有利于节省资源。这块内容到餐技术部在过去一年的工作中，对于核心、非核心的服务都进行了梳理，对于其中可以优化的服务也进行了部分重构。相比年初，很好的降低了资源的成本，业务主机成本的两个主要指标的变化情况如下（备注，后续由于新增其他业务导致成本略有上升）：

##### 大数据成本

##### 

数据行业在互联网的应用目前已经较为成熟，行业主流的数据处理架构都是
Yarn 2.0 或者类似框架，核心的资源消耗主要基于
Container（Vcore+Mem）的计算资源+基于 HDFS 的存储资源消耗这两部分：

第一部分，是存储资源的消耗，行业通用的模型是基于物理 HDFS
或自研的类似存储引擎，这部分主要是指离线 ETL
用来按分区（一般是按时间戳）进行存储的资源，由于数据仓库的核心理念之一是保存"所有"的数据，并在此基础上按照维度建模理论对数据进行预汇总、加和。但是，由于对于模型建设本身的理解深度不同，故在基础数据之上的数据冗余，在很多数据研发人员看来是理所应当的，进而导致存储资源的快速膨胀，这是每个数据团队在管理过程中面临的难题。

在此，到餐研发团队主要采取了两种手段：

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image163.jpeg)
件，同步到前端可以直接访问的介质上，供系统访问。这部分资源有些也是基于
HDFS 的（如
Kylin、HBase），有些需要单独的存储介质，也需要关注其膨胀速度以及存储周期。

第二部分，是计算资源的消耗，主要满足基于复杂规则的分析或者机器学习算法中的计算，也就是实时
ETL 计算和离线ETL 计算的场景（代表性的引擎如 Storm、 Flink 的计算还有
MapReduce
的计算）。这部分计算消耗的资源类似于业务系统，可以参照业务系统的"资源利用率"确定几个指标，进行机器优化或者算法逻辑优化。

##### 分摊成本（一）风控及反爬

在某些公司里，某个技术团队开发的内容，有可能为了服务其他团队业务，比如前文中提到的风控、反爬、广告等，会为各种业务提供基础的技术能力。这时候就涉及到一个重要的概念"分摊"。分摊有两种规则，一种是按"实际用量进行"，另外一种是按照"使用比例"进行，这两种模式之上，可能还有混合计费模式，

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image165.jpeg)
即"按照实际发生的比例进行整体费用的分摊"，做成本控制时，就要清楚地知道这部分成本是按哪种逻辑来进行计算的。

在风控及反爬的实践中，美团的风控及反爬按照整体风控技术团队的总体成本，按比例分摊给业务团队。所以作为业务团队，如果试图降低这部分成本，也要关注两个组成项：一是自己使用的风控及反爬的原子业务数量的绝对值，对每天风控及反爬的总体请求次数是否合理需要进行判断，以保证自己的业务请求量不增加；二是自己业务使用的比例。需要跟相关技术团队一起进行分析，以防止某些场景下，自身业务使用的绝对值下降了，但是因为其他业务绝对值下降的更快，导致自己比例反而上升，进而导致成本上升。

##### 分摊成本（二）安全数仓成本

为了保证各个业务团队之间的离线数据交换，美团集团层面建设了安全数据仓库，用来满足跨团队之间的数据交换。这部分的费用也按照实际发生的资源占比进行
统计，所以同理，为了降低成本，需要关注两个组成项目：一是自己使用的数量，从架构设计上能否将相关数据模型的效率提升、降低空间是关键因素；二是自己

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image167.jpeg)
的使用资源在整体资源的占比，这时候也需要跟相关团队一起努力降低总成本。很多公司的技术团队，也有类似的数据共享仓库或者共建仓库的概念。

##### 分摊成本（三）广告成本

很多互联网公司都有做广告业务的技术团队，广告的形式主要有按点击收费CPC，按时长收费
CPT 等等，这部分分摊的逻辑同上述两者，也是按最终的总费用中
的占比进行分摊。但是这块有一个需要关注的点是，由于广告的业务逻辑并不在到餐自己的业务方，也就是说归到餐研发团队可以控制的部分较小，故在这个过程中需要建立有效的评价体系，来衡量广告分摊的费用，在此采用的指标是"千次曝光成本"和"千元广告收入成本"，这里仅供大家参考。

##### 其他成本

除了以上梳理的项目之外，每月还会有一些新增的成本项目加入进来，团队要保持足够的关注。在实践中会发现某项成本在个别月份突然升高，这时候就要找到是新增加了项目，还是某个指标在业务或者算法上有所调整。

4.  检查（Check）

5.  复盘总结，继续迭代(Act)

## 总结



![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image24.jpeg)


# 数据分析

# 

### 八、美团点评运营数据产品化应用与实践

## 背景

## 挑战

## 方案

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image169.jpeg)


图 1 运营专题整体功能模块图

## 数据仓库层

#### 领域内常见的建模方法

##### ③ DV 模型（DataVault）

DataVault 是 Dan Linstedt 发起的，是一种介于 3NF
和维度建模之间的建模方法。它的设计主要是满足灵活性、可扩展性、一致性和对需求的适应性。它强调建立一个可审计的基础数据层，主要包括
Hub（核心实体）、Link（关系）、 Satellite（实体属性）三个要素。

##### ④ Anchor 模型

Anchor 模型由 Lars. Rönnbäck 提出，是 DataVault
模型的进一步范式化处理，核心思想是只添加、不修改的可扩展模型，Anchor
模型构建的表极窄，类似于 K-V 结构化模型。它主要包括
Anchors（实体且只有主键），Atributes（属性），
Ties（关系），Knots（公用枚举属性)）。Anchor
是应用中比较少的建模方法，只有传统企业和少数几家互联网公司有应用，例如：蚂蜂窝等。

#### 运营专题数据如何构建

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image171.png)


图 2 数据规范抽象示意图

从分布式消息队列中消费 Binlog 和
Click-log，并对埋点数据进行清洗和业务库数据还原，并根据需要增量或全量同步到
Hive，同时积累历史数据并保存。

##### BAS 层主要功能

采用 3NF
建模方法，对整体业务进行概念抽象及适当冗余，在保证数据一致的同时将同属性实体归纳整合到同一逻辑域。BAS
层主要是为了减少数据的不一致，减少存储空间，响应业务系统的变化，避免更新异常。

##### FACT 层主要功能

采用维度建模方法，根据活动特点及事实场景，对代金券、现金券、促销等的事件进一步整合。经过对维度的预处理，在使用信息的时候，不但减少时间成本、提高数据的提取效率，又为用户在
Ad-Hoc 平台查询提供很好的支撑，同时它成为了上层数据应用的关键出口。

##### TOPIC 层主要功能

该层建设不是必须的，是针对业务中个性化诉求，根据需要建设专题数据。服务小范围业务群体和用户，用来支撑核心业务指标外的某一块个性化指标和应用。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image173.png)


图 3 数据仓库模型图

## 多维预计算层

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image175.png)


图 4 构建 cube 示例图

## 中台服务层

## 总体架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image177.jpeg)


图 5 运营专题产品架构图

#### 配置中心

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image179.png)


图 6 配置中心示意图

#### 指标字典

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image181.png)


图 7 指标字典思维导图

#### 规则引擎

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image183.jpeg)


图 8 规则引擎示意图

#### 计算引擎

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image185.jpeg)


图 9 计算引擎示意图

#### 备忘录

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image187.jpeg)


图 10 备忘录示意图

## 数据可视化

#### 趋势对比

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image189.jpeg)


图 11 产品趋势对比图

#### 降维操作

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image191.jpeg)


图 12 产品降维操作图

#### 指标对比

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image193.jpeg)


图 13 产品指标对比图

#### 多维查询

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image195.jpeg)


图 14 产品多维查询图

## 总结

### 九、流量运营数据产品最佳实践------美团旅行流量罗盘

## 背景

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image197.jpeg)


图 1 产品结构图

**挑战**

**解决思路**

**解决方案**

体系架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image199.jpeg)


图 2 流量罗盘体系架构

具体方案

## 公共维度

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image201.jpeg)


图 3 公共维度来源图

## 主题宽表层

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image203.jpeg)


图 4 口径处理模块

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image205.jpeg)


图 5 主题模型计算流程

## 数据应用层

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image207.jpeg)


图 6 应用层计算流程

## 后台服务层

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image209.jpeg)


图 7 配置模块交互

前端展示登录用户具有权限的业务线和平台的入口配置信息。当增加一条配置信息时，配置服务会从入口信息维表中读取对应业务线和平台下的所有可配置的入口信息，配置完入口对应支持的指标及各指标支持的基本维度信息后会将配置信息存入入口配置表。当入口的状态修改为在线状态后，在查询引擎的入口维度中才可以展示此入口维度。

查询模块负责根据用户提交的查询请求中的维度信息，执行查询，返回前端结果。用户提交的请求中如果包含入口信息，会查询配置模块中对应入口配置的指标中是否支持对应维度的查询，若不支持将不对此维度进行限制。如果前端提交的查询请求中不包含入口信息，查询的指标为各业务线设定的默认的指标信息。

查询引擎也会受权限的控制。此模块根据业务线、平台和终端三部分共同配置权限，只有具有某业务线下某个平台和某个终端的权限时，用户方可进行查询服务。其中查询请求流程如图
8 所示：

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image211.jpeg)


图 8 查询服务流程图

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image213.jpeg)


图 9 任务拆分指标计算

**评价指标**

**总结**

**展望**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image215.png)


图 10 产品架构图



![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image24.jpeg)


# 数据治理

十、美团配送数据治理实践

大数据时代的到来，让越来越多的企业看到了数据资产的价值。将数据视为企业的重要资产，已经成为业界的一种共识，企业也在快速探索应用场景和商业模式，并开始建设技术平台。

但这里要特别强调一下，如果在大数据"拼图"中遗忘了数据治理，可能再多的技术投入也是一种徒劳。因为没有数据治理这一环节，其带来后果往往是：随处可见的数据不统一，难以提升的数据质量，难以完成的模型梳理，难以保障的数据安全等等，源源不断的基础性数据问题会进一步产生，进而导致数据建设难以真正发挥其商业价值。

因此，消除数据的不一致性，建立规范的数据标准，提高数据治理能力，实现数
据安全共享，并能够将数据作为企业的宝贵资产应用于业务、管理、战略决策中，发挥数据资产价值变得尤为迫切和重要，数据治理呼之欲出。本文将介绍美团配送技术团队在数据治理方面的一些探索和实践，希望能够对大家有所启发和帮助。

1.  **如何理解数据治理**

2.  **要达成的目标**

3.  **何时进行数据治理**

在数仓迭代阶段，我们希望通过架构治理改变前期开发的"烟囱式"模型，消除冗余，提升数据一致性。并且随着数仓中管理的数据越多，数据安全和成本问题也变得越发重要。所以在该阶段，我们在产研层面逐步开展架构治理、资源治理和安全治理。在架构治理方面，我们明确了数仓中各层和各主题的职责和边界，构建一致的基础数据核心模型，并制定一系列的指标定义规范来确保指标的清晰定义，并基于业务迭代来不断完善和迭代相应的模型和规范。在资源治理方面，我们通过对不同层级的数据采用不同生命周期管理策略，确保用最少的存储成本来满足最大的业务需求。在安全治理方面，我们通过制定一系列的数据安全规范来确保数据的使用安全。

在能力沉淀阶段，我们基于前两个阶段所做的业务和技术沉淀，将前期一系列规范形成标准，从业务到产研，自上而下地推动数据治理，并通过建立相应的组织、流程和制度来保障标准在该阶段的全面落地实施，并通过建设数据治理平台来辅助更高质量地执行标准。

4.  **如何开展数据治理**

    1.  定标准，提质量

第一步，主要围绕着业务标准、技术标准、数据安全标准和资源管理标准进行展开。通过业务标准，指导一线团队完成指标的规范定义，最终达成业务对指标认知一致性这一目标；然后通过技术标准来指导研发同学规范建模，从技术层面解决模型扩展性差、冗余多等问题并保障数据一致性；通过安全标准来指导我们加强数据的安全管控，确保数据拿不走、走不脱，针对敏感数据，用户看不懂；通过资源管理标准的制定，帮助我们在事前做好资源预算，在事中做好资源管理，在事后做好账单管理。

### 业务标准

业务标准主要是指标的管理和运营标准，我们主要解决三个问题：指标由谁来定义，指标该如何定义，指标该如何运营。基于这三个问题，我们同时提出了三条原则：

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image217.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image219.jpeg)
指标定义标准

数仓架构以及建模标准

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image221.jpeg)


仓库各层元数据管理标准

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image223.jpeg)


仓库各层生命周期管理策略

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image225.jpeg)


安全标准建设

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image227.jpeg)
管理，我们都需要基于租户和项目组来进行运营，因此，对于业务团队而言，我们只需要将租户和项目组特定职能划分清楚，然后根据不同的职能归属我们的资产，并分配生产该资产所需要的资源。为了方便后续的运营，我们对每个租户和项目组分配确定了责任人，由责任人对运营结果负责。

对业务部门来说，资源管理的关键是对数据资产做清晰的分类，基于数据的分类划分不同的租户和项目组，将数据和租户、项目组实现一一映射。由于租户和项目组都有特定的责任人对其负责，因此，我们通过这种映射关系，不仅实现了资产的隔离，还实现了资产确权（项目组负责人同时对资产负责和运营）。我们整体将数据分为两大类，一是原始数据，包括流到数据中心的数据和日志中心的数据，针对流入数据中心的数据，根据其产生的方式不同，又进一步分为业务数据和流量数据。二是加工数据，对应着数据团队的仓库建设和其他团队的集市建设。基于上述的描述，针对资源管理，我们做了如下划分和确权：

资源划分与管理

2.  重实施，保落实

第二步，落实第一步的标准，完成数据治理第一阶段的目标，实现存量数据"由乱到治"，并完成相应组织和工具的建设，为实现第二阶段"行不逾矩"这一目标提供工具和组织能力。在此过程中，主要分成三个方面的治理工作：第一，架构模型"由乱到治"的治理，消除模型冗余、跨层引用和链路过长等问题，在架构上保证模型的稳定性和数据一致性；第二，元数据"由乱到治"的治理，实现指标的标准定义、技术元数据的完整采集并建立指标与表、字段的映射关系，彻底解决指标认知一致性，以及用户在使用数据过程中的"找数难"等问题；第三，围绕着隐私安全和共享安全加强数据的安全管控来实现数据走不脱、拿不走，以及隐私数据看不懂这一目标。

### 架构治理

总结起来，架构方面的治理主要是解决两个问题：第一，模型的灵活性，避免需求变更和业务迭代对核心模型带来的冲击，让
RD
深陷无休止的需求迭代中；第二，数据一致性，消除因模型冗余、跨层引用等问题带来的数据一致性问题。

### 模型灵活性

配送解决的是效率、成本和体验三者之间的平衡问题，即在满足一定用户体验的条件下，如何提升骑手配送效率，服务更多的商家，以及如何管控骑手，降低配送成本。抽象到数据层面，基本上反映为上游包裹来源的变化、配送对外提供服务的变化以及对内业务管控的变化。为屏蔽业务迭代给核心模型带来的冲击，我们通过对外封装包裹属性和对内封装运单属性，抽象出包裹来源、提供服务、业

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image229.jpeg)


包裹事实分配到运单明细构造单一运单模型

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image231.jpeg)


桥接表自适配组织层级灵活变动

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image233.jpeg)
在精细化分析的场景下，业务会有分时段、分距离段以及分价格段的数据分析诉求。我们以分时段为例，有晚高峰、午高峰、下午茶等不同的分时段，不同的业务方对同一个时段的定义口径不同，即不同的业务方会有不同的分时段策略。为解决该场景下的分析诉求，我们在事实表中消除退化维度，将原来封装到事实表的时段逻辑迁移到维度表中，并将事实表中的时间进行按特定的间隔进行刻度化作为维表中的主键，将该主键作为事实表的外键。这样，针对业务不同的时间策略需要，我们就可以在维表中进行配置，避免了重复调整事实表和反复刷数的问题。即通过将时间、价格、距离事实刻度化，实现灵活维度分析。如下图所示：

通过将时间刻度化，实现灵活分析

### ![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image235.jpeg)数据一致性

数据一致性得不到保障的一个根本原因，是在建模的过程中没有实现业务口径标
签化，并将业务口径下沉到主题层。很多同学在基于需求进行开发时，为实现方便，将新指标口径通过"Case
When"的方式在应用层和中间层进行封装开发，主题层建设不能随着业务的迭代不断完善，RD
在开发过程中会直接引用仓库的快照表在中间层或应用层完成需求开发。久而久之，就会造成数据复用性低下，相同指标的口径封装在不同的应用表来满足不同报表的需求，但随着应用的增多，很难保障相同指标在不用应用表封装逻辑的一致性，数据一致性难以得到保障，同时这种方式还带来两个严重后果：第一，跨层引用增多，数据复用性低下，造成计算和存储成本的浪费；第二，一旦指标口径发生变化，将是一个"灾难"，不仅影响评估是一个问题，而且涉及该指标的应用层逻辑调整对
RD 来说也是一个巨大的挑战。

治理前模型架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image237.jpeg)


治理后模型架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image239.jpeg)


指标注册流程

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image241.jpeg)


配送业务模式抽象

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image243.jpeg)


元数据建设架构图

的关系，实现了物理表的清晰归属，打通其与生产元数据的关系，为其加上了物理表查询热度、资源消耗、查询密级等生产使用信息，打通其与指标、维度和应用的对应关系，为上层的取数应用建立了完备的元数据。血缘元模型以血缘为中心，不仅构建了从上游业务表到仓库离线表的物理血缘，而且打通了仓库离线表到下游对应报表的血缘，为后续的影响评估构建了完备的元数据基础。

### 元数据服务

统一元数据服务（OneService），主要提供两类元数据服务，提供查询表、指标、维度基本信息的基础元数据服务以及查询表级血缘、字段级血缘的血缘服务。

### 元数据应用

主要孵化出了三个产品，以"找数、理解数、影响评估"为应用场景的数据地图

（Wherehows），以"取数、数据可视化"为应用场景的数据可视化（QuickSight），以及以管理审计为目的的管理审计报表。

### 4.2.3 安全治理

安全治理主要加强了敏感数据的安全治理和数据共享环节的安全治理。通过对隐私数据的安全治理，不仅要保证其在存储环节的不可见性，而且还要保证在其使用环节对用户进行双重鉴权，字段的密级鉴权和解密的密钥鉴权；通过对数据共享环节的安全治理，我们在数据分级分类的基础上，使数据的权限控制从表级权限控制扩展到行级权限控制。

### 敏感数据安全治理

### 

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image245.jpeg)


针对敏感数据的使用安全，我们通过对敏感字段的权限控制和对解密密钥的权限控制，来实现敏感数据使用安全这一目标。针对单独抽取的敏感数据，我们除了针对敏感数据设置其相应的密级确保敏感数据的权限管控外，还基于"暗语"的加密方式为每个项目组分配一个相同的密钥，并且将该密钥存放到与
Hadoop集群集成的 KMS
进行管理（确保支撑离线计算的高并发），确保解密时实现密钥的权限管控。

### 共享环节安全治理

针对共享环节的安全治理，我们主要是在数据生产环节完成数据的分级分类和数据确权，在数据的使用环节完成数据的表级权限控制和行级权限控制。确保数据在使用环节规范的审批流转，权限开放以后的安全审计，保证数据走不脱。

首先，我们在生产环节 B3、B2、B1 层数据按照主题或实体 C
层数据按照应用方向进行逻辑划分，并设定资源的密级和权限负责人。特别地为实现
B3
层数据在查询环节可按照业务线进行权限管控这一目标（即行级鉴权），针对
B3
层数据，我们标记该数据需要在查询环节进行行级权限管控，标记使用行级鉴权所需的字段和该字段对应的枚举值。

其次，在使用环节，我们按照资产密级和使用人角色完成数据的审批流转，实现数据的安全共享。

第三，针对 B3
层数据，审计是否设置了行级权限管控。在数据开放时是否存在越权使用的情况，以及针对即将离职员工加强数据的使用审计，保证数据走不脱。

在数据"由乱到治"的治理过程中，我们不仅实现了存量数据的"由乱到治"，并且在此过程中沉淀出了一系列的建模方法论、工具，并建立了相应的安全小组和指标运营组织。同时，我们为后续增量数据治理确保数据建设"行不逾矩"，提供了强有力的组织保障、稳定的辅助工具和严格的执行标准。在数据治理的第二阶段实现增量数据的"行不逾矩"的过程中，我们主要围绕大数据架构审计、大数据安全与隐私管理审计、大数据质量管理审计和大数据生命周期管理审计这四方面的工作展开，保障治理工作的持续进行，不断提高了组织的治理水平。

5.  **工具简介**

    1.  数据地图（Wherehows）

数据地图作为元数据应用的一个产品，聚焦于数据使用者的"找数"场景，实现检索数据和理解数据的"找数"诉求。我们通过对离线数据集和在线数据集的元数据刻画，满足了用户找数和理解数的诉求，通过血缘图谱，完成物理表到产品的血缘建设，消除用户人肉评估的痛苦。

### 离线数据场景

1.关键字检索和向导查询共同解决了"找数据"的问题：大部分的检索数据场景下，数据使用者都可以通过关键字检索来得到匹配结果。剩下的一小部分场景，例如，对于新人入职后如何了解整个数仓和指标的体系（数仓分几层，每层解决什么问题，都孵化出什么模型；整个指标、维度体系都是怎么分类，有哪些指标和维度），这部分场景可以使用向导查询功能。向导查询相当于分类查询，将表和指标按照业务过程进行分类，用户可以按照分类逐步找到想要的表或指标。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image247.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image248.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image251.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image253.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image254.jpeg)


4.我们通过评论问答功能，帮助用户可以快速得到问题反馈：如果用户看了信息后还是感到有问题，"Wherehows"提供评论问答的功能，用户通过这个功能可以进行提问，会有相应的负责人进行回复。对于重复问反复问的问题，用户通过查看其它人的提问和回复就能找到答案。并且负责人还会定期的将问答信息沉淀到对应的元数据里，不断地对元数据进行补充和完善。

### 业务数据场景

### 

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image257.jpeg)
业务数据场景主要想解决的一个问题是，如何知道一个业务表（MySQL
表）有没有同步到数仓。如果没有同步，能够找谁进行同步。因为已经打通"业务表
-\>数仓表 -\>
产品"三者之间的血缘关系，我们能够轻松解决业务数据场景的问题。

### 生产评估场景

在日常数据生产工作中，我们经常需要对表进行影响评估、故障排查、链路分析等工作，这些工作如果靠纯人工去做，费时费力。但现在我们已经打通了"业务表/字段
-\> 数仓表/字段 -\> 产品"三者之间的血缘关系，就能够在 10
分钟内完成评估工作。对于不同的场景，血缘链路提供了两个便捷的功能：过滤和剪枝。例如，某个表逻辑需要修改，需要看影响哪些下游表或产品？应该要通知哪些
RD 和
PM？这种情况下，血缘工具直观地显示影响了哪些负责人和产品，以及这个表的下游链路。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image259.jpeg)


有些表的链路很长，整个血缘关系图很大，这样会导致用户定位信息或问题。所以血缘工具提供了剪枝的功能，对于没用的、不想看到的分支可以剪掉，从而让整个链路变得更加直观。

2.  数据可视化（QuickSight）

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image261.jpeg)


用户、指标池与数据集间的关系

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image263.png)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image264.jpeg)


指标加工组件

6.  **总结与展望**

十一、美团酒旅数据治理实践

**一、背景**

1.  为什么要做数据治理

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image267.jpeg)


2.  需要治理哪些问题

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image269.jpeg)


3.  美团酒旅数据现状

4.  治理目标

**二、数据治理实践**

1.  数据治理策略

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image271.jpeg)


2.  标准化和组织保障

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image273.jpeg)


数据标准化包括三个方面：一是标准制定；二是标准执行；三是在标准制定和执行过程中的组织保障，比如怎么让标准能在数据技术部门、业务部门和相关商业分析部门达成统一。

从标准制定上，我们制定了一套覆盖数据生产到使用全链路的数据标准方法，从数据采集、数仓开发、指标管理到数据生命周期管理都建立了相应环节的标准化的研发规范，数据从接入到消亡整个生命周期全部实现了标准化。

### 组织保障

根据美团数据管理分散的现状，专门建立一个职能全面的治理组织去监督执行数据治理工作的成本有点太高，在推动和执行上，阻力也会比较大。所以，在组织保障上，我们建立了委员会机制，通过联合业务部门和技术部门中与数据最相关的团队成立了数据管理委员会，再通过委员会去推动相关各方去协同数据治理的相关工作。

业务部门的数据接口团队是数据产品组，数据技术体系是由数据开发组负责建设，所以我们以这两个团队作为核心建立了业务数据管理委员会，并由这两个团队负
责联合业务部门和技术部门的相关团队，一起完成数据治理各个环节工作和流程的保障。组织中各个团队的职责分工如下：

**数据管理委员会**：负责数据治理策略、目标、流程和标准的制定，并推动所有相关团队达成认知一致。
**业务数据产品组**：负责数据标准、需求对接流程、指标统一管理、数据安全控制以及业务方各部门的协调推动工作。
**技术数据开发组**：

3.  技术系统

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image275.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image277.png)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image279.png)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image280.png)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image281.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image283.jpeg)


在查询解析过程中，经常出现指标绑定了多个底层数据表的情况，此时需要我们手动的选一个物理模型作为指标生产的底层数据。但问题是，如果一个指标对应的模型太多，每次解析都需要手动指定，研发人员不确定选择哪个模型的性能最好。另外，随着物理模型的增多，大量旧的指标配置的关联模型不是最优解，就需要手动优化更改。为了解决这个问题，指标管理系统增加了智能解析模块，在选择智能模式查询时，系统会根据指标管理模型的数据量、存储性能和查询次数等信息自动选取最优的物理模型。

### 3.1.3 统一数据服务（One Service）

数据仓库对外提供数据的需求越来越多，除了管理层、分析师和产品运营同学使用数据产品和报表外，数据还需要提供到各个业务系统中使用。常用的提供数据的方式主要包括同步数据表、提供
SQL 和为下游服务开发定制化 API 接口等方式，但存在以下几个方面的问题：

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image285.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image287.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image289.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image291.jpeg)
数据团队通过数据资产信息的系统化的方式建设易用的数据检索产品，帮助用户更快捷、更方便地找到数据，并指导用户正确地使用数据，提高数据信息的易用性，以此减少数据工程师的数据答疑和运维时间。实现策略是通过用户的问题分类，通过数据信息系统化的方式分类解答
80%的问题，最后少量的问题透传到研发人员再进行人工答疑。系统化方式主要分两层，数据使用智能和数据答疑机器人。

### 3.2.2 数据使用指南系统

数据使用指南的定位是业务数据信息的知识白皮书，提供最新、最全、最准确的指标口径、项目指标体系、数据表用法等信息，以简洁、流畅的操作支持数据指南中的内容及时更新，降低业务方的数据答疑和数据使用成本。

数据使用指南通过把业务场景和数据使用场景打通，从业务场景分析到使用到的数据表、指标和数据产品打通，在系统中能够快速找到数据表、指标定义、数据查询
SQL、指标所在数据产品等信息，一站式解决数据查找、使用和分析的全部场景。主要功能包括指标信息和数据表信息及使用。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image293.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image295.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image296.jpeg)


-   **存储资源**

-   **日志采集资源**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image299.jpeg)


4.  衡量指标

    2.  ### ![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image301.jpeg)衡量指标保障数据治理

根据 PDCA
原则，将数据治理作为日常的运营项目做起来，底层依赖数据指标体系进行监控，之上从发现问题到提出优化方案，然后跟进处理，再到日常监控，构成一个完整的循环。

5.  治理效果总结

**三、未来规划**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image305.png)


十二、DataMan-美团旅行数据质量监管平台实践

**背景**

**挑战**

**解决思路**

整体框架

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image307.jpeg)


图 1 质量监控平台整体框架图

建设方法

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image309.jpeg)


图 2 数据质量 PDCA 流程图

质量检核标准

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image311.jpeg)


图 3 质量检核标准图

监管核心点

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image313.jpeg)


图 4 数据质量监管功能图

管理流程

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image315.jpeg)


图 5 数据质量流程图

**技术方案**

总体架构

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image317.jpeg)


图 6 质量监管 DataMan 总体架构图

技术框架

## 前后端技术

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image319.jpeg)


图 7 技术架构图

## Zebra 中间件

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image321.jpeg)


图 8 Zebra 架构图

数据模型

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image323.jpeg)


图 9 数据流向层级图

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image325.jpeg)


图 10 数据质量集市模型图

**系统展示**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image327.jpeg)


图 11 系统效果图

个人工作台

离线监控

实时监控

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image329.jpeg)


图 12 实时作业运行监控图

推荐信息

公共账号

故障处理

**总结**



![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image24.jpeg)


# 数据同步

十三、美团 DB 数据同步到数据仓库的架构与实践

**背景**

**整体架构**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image331.png)


**Binlog 实时采集**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image333.png)


**离线还原 MySQL 数据**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image335.png)


Kafka2Hive

对Camus 的二次开发

Kafka 上存储的 Binlog 未带 Schema，而 Hive 表必须有
Schema，并且其分区、字段等的设计，都要便于下游的高效消费。对 Camus
做的第一个改造，便是将 Kafka 上的 Binlog 解析成符合目标 Schema
的格式。

对 Camus 做的第二个改造，由美团的 ETL
框架所决定。在我们的任务调度系统中，目前只对同调度队列的任务做上下游依赖关系的解析，跨调度队列是不能建立依赖关系的。而在
MySQL2Hive 的整个流程中，Kafka2Hive
的任务需要每小时执行一次（小时队列），Merge
任务每天执行一次（天队列）。而 Merge任务的启动必须要严格依赖小时
Kafka2Hive 任务的完成。

为了解决这一问题，我们引入了 Checkdone 任务。Checkdone
任务是天任务，主要负责检测前一天的 Kafka2Hive
是否成功完成。如果成功完成了，则 Checkdone 任务执行成功，这样下游的
Merge 任务就可以正确启动了。

Checkdone 的检测逻辑

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image337.jpeg)
此外，由于 Camus 本身只是完成了读 Kafka
然后写 HDFS 文件的过程，还必须完成对 Hive
分区的加载才能使下游查询到。因此，整个 Kafka2Hive 任务的最后一步是加载
Hive 分区。这样，整个任务才算成功执行。

每个 Kafka2Hive 任务负责读取一个特定的 Topic，把 Binlog 数据写入
original_binlog 库下的一张表中，即前面图中的
original_binlog.\*db\*，其中存储的是对应到一个 MySQL DB 的全部
Binlog。

上图说明了一个 Kafka2Hive 完成后，文件在 HDFS 上的目录结构。假如一个
MySQL DB 叫做user，对应的 Binlog 存储在original_binlog.user
表中。ready目录中，按天存储了当天所有成功执行的 Kafka2Hive
任务的启动时间，供 Checkdone 使用。每张表的
Binlog，被组织到一个分区中，例如 userinfo 表的 Binlog，存储在
table_name=userinfo 这一分区中。每个 table_name 一级分区下，按 dt
组织二级分区。图中的 xxx.lzo 和 xxx.lzo.index 文件，存储的是经过 lzo
压缩的 Binlog 数据。

Merge

Merge 流程举例

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image339.png)


**实践一：分库分表的支持**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image341.png)


首先，在 Binlog 实时采集时，我们支持把不同 DB 的Binlog
写入到同一个Kafka Topic。用户可以在申请 Binlog
采集时，同时勾选同一个业务逻辑下的多个物理 DB。通过在 Binlog
采集层的汇集，所有分库的 Binlog 会写入到同一张
Hive表中，这样下游在进行 Merge 时，依然只需要读取一张 Hive 表。

第二，Merge
任务的配置支持正则匹配。通过配置符合业务分表命名规则的正则表达式，Merge
任务就能了解自己需要聚合哪些 MySQL 表的
Binlog，从而选取相应分区的数据来执行。

这样通过两个层面的工作，就完成了分库分表在 ODS 层的合并。

这里面有一个技术上的优化，在进行 Kafka2Hive
时，我们按业务分表规则对表名进行了处理，把物理表名转换成了逻辑表名。例如
userinfo123 这张表名会被转换为 userinfo，其 Binlog 数据存储在
original_binlog.user 表的 table_name=userinfo
分区中。这样做的目的是防止过多的 HDFS 小文件和 Hive
分区造成的底层压力。

**实践二：删除事件的支持**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image342.png)


**总结与展望**

# Doris 和 Kylin 实践

十四、Apache Doris 在美团外卖数仓中的应用实践

**序言**

**数仓交互层引擎的应用现状**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image344.jpeg)


**汇总数据的交互**

### ![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image346.jpeg)明细数据的交互

业务分析除了宏观数据之外，对明细数据查询也是一种刚需。通常大家会选择
MySQL 等关系型 DB
作为明细数据的快速检索查询，但当业务成长较快时，很快就会遇到性能瓶颈，并且运维成本也很高。例如，大数据量的同步、新增字段、历史数据更新等操作，它们的维护成本都非常高。

### 外卖运营业务特点

美团的使命是"帮大家吃得更好，生活更好"。外卖业务为大家提供送餐服务，连接商家与用户，这是一个劳动密集型的业务，外卖业务有上万人的运营团队来服务全国几百万的商家，并以"商圈"为单元，服务于"商圈"内的商家。"商圈"是一个组织机构维度中的最小层级，源于外卖组织的特点，"商圈"及其上层组织机构是一个变化维度，当"商圈"边界发生变化时，就导致在往常日增量的业务生产方式中，历史数据的回溯失去了参考意义。在所有展现组织机构数据的业务场景中，组织机构的变化是一个绕不开的技术问题。此外，商家品类、类型等其它维度也存在变化维的问题。如下图所示：

**数据生产面临的挑战**

**解决方案：引入 MPP 引擎，数据现用现算**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image348.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image350.jpeg)
**双引擎下的应用场景适配问题**

架构上通过 MOLAP+ROLAP 双引擎模式来适配不同应用场景，如下图所示：

### 技术权衡

**MOLAP**：通过预计算，提供稳定的切片数据，实现多次查询一次计算，减轻了查询时的计算压力，保证了查询的稳定性，是"空间换时间"的最佳路径。实现了基于
Bitmap 的去重算法，支持在不同维度下去重指标的实时统计，效率较 高。
**ROLAP**：基于实时的大规模并行计算，对集群的要求较高。MPP
引擎的核心是通过将数据分散，以实现
CPU、IO、内存资源的分布，来提升并行计算能力。在当前数据存储以磁盘为主的情况下，数据
Scan 需要的较大的磁盘 IO，以及并行导致的高
CPU，仍然是资源的短板。因此，高频的大规模汇总统计，并发能力将面临较大挑战，这取决于集群硬件方面的并行计算能力。传统去重算法需要大量计算资源，实时的大规模去重指标对
CPU、内存都是一个巨大挑战。目前 Doris 最新版本已经支持 Bitmap
算法，配合预计算可以很好地解决去重应用场景。

### ![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image352.jpeg)业务模型适配

**MOLAP**： 当业务分析维度相对固化，并在可以使用历史状态时，按照时间进
行增量生产，加工成本呈线性增长状态，数据加工到更粗的粒度（如组织单元），减少结果数据量，提高交互效率。如上图所示，由
A 模型预计算到 B 模型，使用 Kylin 是一个不错的选择。

**ROLAP**： 当业务分析维度灵活多变或者特定到最新的状态时（如上图 A
模型中，始终使用最新的商家组织归属查看历史），预计算回溯历史数据成本巨大。在这种场景下，将数据稳定在商家的粒度，通过现场计算进行历史数据的回溯分析，实现现用现算，可以节省掉预计算的巨大成本，并带来较大的应用灵活性。这种情况下适合
MPP 引擎支撑下的 ROLAP 生产模式。

**MPP 引擎的选型**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image354.jpeg)


**Doris 简介及特点**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image356.jpeg)


整体架构

**Doris 在外卖数仓中的应用效率**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image358.jpeg)


**准实时场景下的应用**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image360.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image362.jpeg)


**Doris 引擎在美团的重要改进**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image364.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image366.jpeg)
于是我们在 Doris 中实现了第一个优化：Join
谓词下推的传递性优化（MySQL和 TiDB 中称之为 Constant
Propagation）。Join 谓词下推的传递性优化是指：基于谓词 t1.id = t2.id
和 t1.id = 1, 我们可以推断出新的谓词 t2.id = 1，并将谓词 t2.id = 1
下推到 t2 的 Scan 节点。 这样假如 t2
表有数百个分区的话，查询性能就会有数十倍甚至上百倍的提升，因为 t2
表参与 Scan 和 Join 的数据量会显著减少。

### 查询执行多实例并发优化

如上图所示，Doris 默认在每个节点上为每个算子只会生成 1
个执行实例。这样的话，如果数据量很大，每个执行实例的算子就需要处理大量的数据，而且无法充分利用集群的
CPU、IO、内存等资源。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image368.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image370.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image372.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image373.jpeg)


图中是 6 行数据在 2 个 BE 节点上计算的示意图

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image376.jpeg)


**总结与思考**

十五、Apache Kylin 的实践与优化

**背景**

**问题与目标**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image378.png)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image380.jpeg)


**优化前提-原理解读**

预计算

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image382.png)


By-layer 逐层算法

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image384.jpeg)


**过程分析-层层拆解**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image386.jpeg)


构建引擎选择

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image388.jpeg)


读取源数据

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image390.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image391.jpeg)


构建字典

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image394.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image396.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image397.jpeg)


分层构建

### ![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image400.png)Job 阶段

Job 个数为 By-layer 算法树的层数，Spark 将每层结果数据的输出，作为一个
Job。如下图所示：

### Stage 阶段

每个 Job 对应两个 Stage
阶段，分为读取上层缓存数据和缓存该层计算后的结果数据。如下图所示：

-   Task 个数计算公式：Min(MapSize/cut-mb ，MaxPartition)
    ；Max(MapSize/cut-mb ，MinPartition)

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image402.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image403.jpeg)


文件转换

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image406.jpeg)


**实施路线-由点及面**

交易试点实践

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image408.jpeg)


实践结果对比

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image410.png)


**成果展示**

资源整体情况

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image412.png)


SLA 整体达成率

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image414.png)


**展望**

# A/B 测试

十六、美团配送 A/B 评估体系建设实践

2019 年 5 月 6
日，美团点评正式推出新品牌"美团配送"，发布了美团配送新愿景："每天完成一亿次值得信赖的配送服务，成为不可或缺的生活基础设施。"现在，美团配送已经服务于全国
400 多万商家和 4 亿多用户，覆盖 2800 余座

市县，日活跃骑手超过 70 万人，成为全球领先的分钟级配送网络。

即时配送的三要素是"效率"、"成本"、"体验"，通过精细化的策略迭代来提升效率，降低成本，提高体验，不断地扩大规模优势，从而实现正向循环。但是，策略的改变，不是由我们随便"拍脑袋"得出，而是一种建立在数据基础上的思维方式，数据反馈会告诉我们做的好不好，哪里有问题，以及衡量可以带来多少确定性的增长。而
A/B-test
就是我们精细化迭代的一个"利器"，通过为同一个迭代目标制定两个或多个版本的方案，在同一时间维度，让组成成分相同

（或相似）的 A/B
群组分别采用这些版本，然后收集各群组的体验数据和业务数据，最后分析、评估出最好的版本，帮助我们作出正确的决策，使迭代朝着更好的方向去演进。基于此，构建一个适用于配送业务的
A/B 平台就应运而生了。

1.  **A/B 平台简介**

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image416.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image418.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image420.jpeg)


2.  **为什么要强调评估体系建设**

    1.  分流业务场景需要

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image422.jpeg)
![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image423.jpeg)


这种是面向 C 端用户进行流量选择的传统 A/B
实验，采用上述的分流方式基于这样的假设：参与实验的流量因子是相互独立的、随机的，服从独立同分布。但是，配送业务场景下的
A/B
实验，涉及到用户、骑手、商家三端，请求不独立，策略之间相互影响并且受线下因素影响较大。传统
A/B
实验的分流方式，无法保证分出的两个群组实验组和对照组的流量都是无差别的，无法避免因流量分配不平衡而导致的
A/B
群组差异过大问题，很容易造成对实验结果的误判。为满足不同业务场景的诉求，我们的
A/B 平台建设采取了多种分流策略，如下图所示：

针对策略之间的相互影响、请求不独立场景下的 A/B
实验，我们采取限流准入的分流方式，针对不同的实验，选取不同的分流因子。在实验前，我们通过
AA分组，找出无差别的实验组和对照组，作为我们实验分流配置的依据，这种分流方式要求我们要有一套完整刻画流量因子的指标体系，只要刻画流量因子的指标间无统计显著性，我们就认为分出的实验组和对照组无差别。

2.  业务决策的重要依据

```{=html}
<!-- -->
```
3.  **A/B 评估体系构建**

    1.  权威完备的指标体系

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image426.jpeg)
治理指标强调的是指标的权威性和生产的规范性，而探索性指标强调的是指标的多样性和生产的灵活性。在评估体系中要实现这两类指标的统一，既要包含用于说明实验效果的治理指标，又要包含帮助实验者更好迭代实验所需的探索指标。

为实现上述的统一，指标层面要有分级运营的策略：治理指标按照业务认知一致性和算法内部认知一致性分别定级为
P0、P1，这一类指标在生产前必须要有严格的注册、评审，生产环节需要交给独立的第三方团队（数据团队）生产，保证指标的权威性，产出后打通指标与字段的映射关系，对用户屏蔽底层实现逻辑；对于探索性指标，定级为
P2，强调的是生产的灵活性和快速实现，因此，它的生产就不宜带有指标注册和评审等环节。为保证其快速实现，希望基于物理表和简单的算子配置就可以实现效果分析时即席查询使用。基于如上的问题拆解，我们进行了如下的架构设计：

### 数据集成

### 

为了支持监控和分析，在数据集成环节，我们集成了实验配置数据、业务数据和染色数据，以便实验者在效果评估环节不仅可以查看流量指标（PV、UV
和转化率），也可以深入探索策略变动对业务带来的影响。对于那些在实验配置环节不能确定流量是否真正参加实验的场景（例如：选择了特定区域进行实验，该区域产生的单只有满足特定条件时才能触发实验），我们不能直接通过限制确定的区域来查看业务指标。因为，此时查看的指标并不是真正参与实验的流量所对应的指标。因此在数据集成环节，我们同时将实验前的实验配置数据和实验中的染色数据（针对每个参与实验的流量，每次操作所产生的数据，都会打上实验场景、实验组以及具体的分组标记，我们该数据为染色数据）同步到数仓。在数据基建环节，将业务数据模型和染色数据模型通过流量实体作为关联条件进行关联，构建实验粒度模型。

### 数据基建

在数据基建层，我们基于指标分级运营的思路，由数据团队和算法团队分别构建
实体粒度（区域、骑手、GeoHash）和实验粒度的实体宽表模型，以满足 P0/P1
指标和 P2
指标的诉求；为实现指标的规范化建设和灵活建设的统一，在物理模型和对外提供应用的指标池之间，我们提供了元数据管理工具和模型配置工具，从而实现离线数据快速接入评估体系的指标池。由数据团队建设的实体宽表模型，对应着治理指标（P0/P1
指标），必须在生产后通过元数据管理工具完成指标与物理字段的映射，将指标的加工口径封装在数据层，对用户屏蔽物理实现，确保治理指标的一致性。由算法团队独立建设的实体宽表模型，对应着挖掘指标（P2指标），为确保其接入评估体系指标池的灵活性和方便性，我们在数据基建环节，

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image428.jpeg)
通过标签的形式对指标口径做部分封装，在模型配置环节完成指标逻辑的最终加工。

### 元数据管理

元数据管理层，是实现指标权威性的关键。治理指标在本层实现注册、评审，达到业务认知一致性和算法内部认知一致性的目的。同时，本层还完成了治理指标与数据基建层物理模型之间的绑定，为后续的模型配置建立基础。

### 模型配置

模型配置工具，是打通物理模型与评估指标池的桥梁，它通过输入组件、操作组件和应用组件，将离线数据接入到评估体系中，满足实验前
AA 分组和实验后 AB
评估的需求。首先，输入组件可以对应不同的数据源，既可以接入治理的离线指标，也可以接入特定库下的物理表。其次，操作组件提供了分组操作、算子操作、过滤操作和测试操作，通过分组操作，确定模型包含的维度；通过算子操作，将算子作用在指标或标签字段上，在取数环节实现指标的二次计算；通过过滤操作，实现数据的过滤；通过测试操作，保证模型配置质量。最后，应用组件可以将配置的模型注册到不同的应用上，针对
A/B 场景主要是 AA 分组和 AB 评估。具体接入流程如下图所示：

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image430.jpeg)


2.  科学权威的评估方式

无论是实验前确保实验组和对照组流量无显著性差异，还是实验后新策略较旧策略的指标变动是否具有统计上的显著性，无一例外，它们都蕴含着统计学的知识。接下来，我们重点论述一下
A/B 实验所依赖的统计学基础以及如何依据统计学理论做出科学评估。

1.  ### 假设检验

    1.  **两个假设**

A/B
测试是一种对比试验，我们圈定一定的流量进行实验，实验结束后，我们基于实验样本进行数据统计，进而验证实验前假设的正确性，我们得出这一有效结论的科学依据便是假设检验。假设检验是利用样本统计量估计总体参数的方法，在假设检验中，先对总体均值提出一个假设，然后用样本信息去检验这个假设是否成立。我们把提出的这个假设叫做原假设，与原假设对立的结论叫做备择假设，如果原假设不成立，就要拒绝原假设，进而接受备择假设。

### 两类错误

对于原假设提出的命题，我们需要作出判断，要么原假设成立，要么原假设不成立。因为基于样本对总体的推断，会面临着犯两种错误的可能：第一类错误，原假设为真，我们却拒绝了；第二类错误，原假设为伪，我们却接受了。显然，我

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image432.png)
们希望犯这两类错误的概率越小越好，但对于一定的样本量
n，不能同时做到犯这两类错误的概率很小。

在假设检验中，就有一个对两类错误进行控制的问题。一般来说，哪一类错误所带来的后果严重、危害越大，在假设检验中就应该把哪一类错误作为首要的控制目标。在假设检验中，我们都执行这样一个原则，首先控制犯第一类错误的概率。这也是为什么我们在实际应用中会把要推翻的假设作为原假设，这样得出的结论更具说服力（我们有足够充分的证据证明原来确定的结论是错误的），所以通常会看到，我们把要证明的结论作为备择假设。

### T 检验

常见的假设检验方法有 Z 检验、T
检验和卡方检验等，不同的方法有不同的适用条件和检验目标。Z 检验和T
检验都是用来推断两个总体均值差异的显著性水平，具体选择哪种检验由样本量的大小、总体的方差是否已知决定。在样本量较小且总体的方差未知的情况下，这时只能使用样本方差代替总体方差，样本统计量服从
T 分布，应该采用 T 统计量进行检验。T
统计量具体构造公式如下图所示，其中 f 是 T 统计量的自由度，S1、S2
是样本标准差。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image434.jpeg)
T 检验的流程是，在给定的弃真错误概率下（一般取
0.05），依据样本统计量
T是否落在拒绝域来判断接受还是拒绝原假设。实际上在确定弃真错误概率以后，拒绝域的位置也就相应地确定了。使用
T
统计量进行判断的好处是，进行决策的界限清晰，但缺陷是决策面临的风险是笼统的。例如
T=3 落入拒绝域，我们拒绝原假设，犯弃真错误的概率为 0.05；T=2
也落入拒绝域，我们拒绝原假设，犯弃真错误的概率也是
0.05。事实上，依据不同的统计量进行决策，面临的风险也是有差别的。为了精确地反映决策的风险度，我们仍然需要
P 值来帮助业务来做决策。

### 利用 P 值决策

P
值是当原假设为真时，所得到的样本观察结果或更极端的结果出现的概率。如果
P 值很小，说明这种情况发生的概率很小，但是在这次试验中却出现了，根
据小概率原理，我们有理由拒绝原假设，P
值越小，我们拒绝原假设的理由越充分。P
值可以理解为犯弃真错误的概率，在确定的显著性水平下（一般取 0.05）， P
值小于显著性水平，则拒绝原假设。

### 基于假设检验的科学评估

围绕着科学评估要解决的两个问题，实验前，针对圈定的流量使用假设检验加上动态规划算法，确保分出无差别的实验组和对照组；实验后，基于实验前选定的用于验证假设结论的指标，构造
T 统计量并计算其对应的 P 值，依据 P 值帮我们做决策。

### AA 分组

首先看如何解决第一个问题：避免因流量分配不平衡，A/B
组本身差异过大造成对实验结果的误判。为解决该问题，我们引入了 AA
分组：基于实验者圈定的流量，通过 AA
分组将该流量分为无显著性差异的实验组和对照组。我们这样定义无显著性差异这一约束：首先，实验者选取的用于刻画实验流量的指标，在实验组和对照组之间无统计上的显著性（即上节所描述的基于均值的假设检验）；其次，在所分出的实验组和对照组之间，这些指标的差值最小，即一个寻找最优解的过程。从实验者的实验流程看，在实验前，圈定进入该实验的流量，然后确定用于刻画实验流量的指标，最后调用
AA 分组，为其将流量分成合理的实验组和对照组。

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image436.jpeg)


### A/B 效果评估

A/B 效果评估是实验者在实验后，依据评估报告进行决策的重要依据。
因此，我们在实验后的效果评估环节，效果评估要达成三个目标即权威、灵活性和方便。首先，权威性体现在用于作出实验结论所依赖的指标都是经过治理、各方达成一致的指标，并且确保数据一致性，最终通过假设检验给出科学的实验结论，帮助实验者作出正确的判断。其次，灵活性主要体现在采用列转行的形式，按需自动生成报表告别"烟囱式"的报表开发方式。第三，方便主要体现在不仅可以查看用于说明实验效果的指标，还可以选择查看接入到评估体系里的任意指标；不仅可以查看其实验前后对比以及趋势变化，还可以做到从实验粒度到流量实体粒度的下钻。效果如下图所示：

![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image438.jpeg)


![](https://raw.githubusercontent.com/yeyangchen2009/img_bed/master/bigdata/美团数据平台及数仓建设实践/image440.jpeg)




